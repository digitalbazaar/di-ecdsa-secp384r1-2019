<!DOCTYPE html>
<html>
  <head>
    <title>Data Integrity ECDSA Cryptosuites v1.0</title>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <!--
      === NOTA BENE ===
      For the three scripts below, if your spec resides on dev.w3 you can check them
      out in the same tree and use relative links so that they'll work offline,
     -->
    <script src="//www.w3.org/Tools/respec/respec-w3c" class="remove"></script>
    <script class="remove" src="https://w3c.github.io/vc-data-integrity/common.js"></script>

    <script type="text/javascript" class="remove">
      var respecConfig = {
        subtitle: "Achieving Data Integrity using ECDSA with NIST-compliant curves",
        // specification status (e.g. WD, LCWD, NOTE, etc.). If in doubt use ED.
        group: "vc",
        specStatus: "WD",

        // the specification's short name, as in http://www.w3.org/TR/short-name/
        shortName: "vc-di-ecdsa",

        // if you wish the publication date to be other than today, set this
        //publishDate:  "2023-04-18",

        // if there is a previously published draft, uncomment this and set its YYYY-MM-DD date
        // and its maturity status
        // previousPublishDate:  "1977-03-15",
        // previousMaturity:  "WD",

        // if there a publicly available Editor's Draft, this is the link
        edDraftURI: "https://w3c.github.io/vc-di-ecdsa/",
        //latestVersion: "https://www.w3.org/community/reports/credentials/CG-FINAL-di-ecdsa-2019-20220724/",

        // if this is a LCWD, uncomment and set the end of its review period
        // lcEnd: "2009-08-05",

        // if you want to have extra CSS, append them to this list
        // it is recommended that the respec.css stylesheet be kept
        //extraCSS:             ["spec.css", "prettify.css"],

        // editors, add as many as you like
        // only "name" is required
        editors: [{
          name: "Manu Sporny",
          url: "https://www.linkedin.com/in/manusporny/",
          company: "Digital Bazaar",
          companyURL: "https://digitalbazaar.com/",
          w3cid: 41758
        }, {
          name: "Marty Reed",
          url: "https://www.linkedin.com/in/marty-reed-b5b2352/",
          company: "RANDA Solutions",
          companyURL: "https://www.randasolutions.com/",
          w3cid: 127611
        }, {
          name: "Greg Bernstein", url: "https://www.grotto-networking.com/",
          company: "Invited Expert", w3cid: 140479
        }, {
          name: "Sebastian Crane", url: "https://github.com/seabass-labrax",
          company: "Invited Expert", w3cid: 140132
        }],

        // authors, add as many as you like.
        // This is optional, uncomment if you have authors as well as editors.
        // only "name" is required. Same format as editors.

        authors: [{
          name: "Dave Longley", url: "https://digitalbazaar.com/",
          company: "Digital Bazaar", companyURL: "https://digitalbazaar.com/",
          w3cid: 48025
        }, {
          name: "Manu Sporny", url: "https://www.linkedin.com/in/manusporny/",
          company: "Digital Bazaar", companyURL: "https://digitalbazaar.com/",
          w3cid: 41758
        }],

        // extend the bibliography entries
        //localBiblio: webpayments.localBiblio,

        // name of the WG
        //wg:           "W3C Credentials Community Group",

        // URI of the public WG page
        //wgURI:        "https://www.w3.org/community/credentials/",

        // name (with the @w3c.org) of the public mailing to which comments are due
        //wgPublicList: "public-credentials",

        github: "https://github.com/w3c/vc-di-ecdsa/",

        otherLinks: [],

        // URI of the patent status for this WG, for Rec-track documents
        // !!!! IMPORTANT !!!!
        // This is important for Rec-track documents, do not copy a patent URI from a random
        // document unless you know what you're doing. If in doubt ask your friendly neighbourhood
        // Team Contact.
        // wgPatentURI:  "",
        maxTocLevel: 4,
        /*preProcess: [ webpayments.preProcess ],
        alternateFormats: [ {uri: "diff-20111214.html", label: "diff to previous version"} ],
        */
        localBiblio: {
          MULTIBASE: {
            title: "Multibase",
            href: "https://datatracker.ietf.org/doc/html/draft-multiformats-multibase-01",
          },
          MULTICODEC: {
            title: "Multicodec",
            href: "https://github.com/multiformats/multicodec/",
          },
          SECG2: {
            title: "SEC 2: Recommended Elliptic Curve Domain Parameters",
            href: "http://www.secg.org/sec2-v2.pdf",
            date: "January 27, 2010",
            publisher: "Certicom Research"
          },
          "NIST-SP-800-186": {
            title: "Recommendations for Discrete Logarithm-based Cryptography: Elliptic Curve Domain Parameters",
            authors: ["Lily Chen", "Dustin Moody", "Karen Randall", "Andrew Regenscheid", "Angela Robinson"],
            date: "February 2023",
            publisher: "National Institute of Standards and Technology"
          },
          "NIST-SP-800-186": {
            title: "Recommendations for Discrete Logarithm-based Cryptography: Elliptic Curve Domain Parameters",
            authors: ["Lily Chen", "Dustin Moody", "Karen Randall", "Andrew Regenscheid", "Angela Robinson"],
            date: "February 2023",
            publisher: "National Institute of Standards and Technology"
          },
          "NIST-SP-800-57-Part-1": {
            title: "Recommendation for Key Management: Part 1 â€“ General",
            authors: ["Elaine Barker"],
            date: "May 2020",
            publisher: "National Institute of Standards and Technology",
            href: "https://doi.org/10.6028/NIST.SP.800-57pt1r5"
          }
        },
        lint: {"no-unused-dfns": false},
        postProcess: [restrictRefs],
        otherLinks: [{
          key: "Related Specifications",
          data: [{
            value: "The Verifiable Credentials Data Model v2.0",
            href: "https://www.w3.org/TR/vc-data-model-2.0/"
          }, {
            value: "Verifiable Credential Data Integrity v1.0",
            href: "https://www.w3.org/TR/vc-data-integrity/"
          }, {
            value: "The Edwards Digital Signature Algorithm Cryptosuites v1.0",
            href: "https://www.w3.org/TR/vc-di-eddsa/"
          }, {
            value: "The BBS Digital Signature Algorithm Cryptosuites v1.0",
            href: "https://www.w3.org/TR/vc-di-bbs/"
          }]
        }]
      };
    </script>
    <style>
code {
  color: rgb(199, 73, 0);
  font-weight: bold;
}
pre.nohighlight {
  overflow-x: auto;
  white-space: pre-wrap;
}
pre .highlight {
  font-weight: bold;
  color: green;
}
pre .comment {
  font-weight: bold;
  color: Gray;
}
.color-text {
  font-weight: bold;
  text-shadow: -1px 0 black, 0 1px black, 1px 0 black, 0 -1px black;
}
ol.algorithm {
  counter-reset: numsection;
  list-style-type: none;
}
ol.algorithm li {
  margin: 0.5em 0;
}
ol.algorithm li:before {
  font-weight: bold;
  counter-increment: numsection;
  content: counters(numsection, ".") ") ";
}
    </style>
  </head>
  <body>
    <section id="abstract">
      <p>
This specification describes a Data Integrity Cryptosuite for use when
generating a digital signature using the Elliptic Curve Digital Signature
Algorithm (ECDSA).
      </p>
    </section>

    <section id="sotd">
      <p>
This is an experimental specification and is undergoing regular revisions. It
is not fit for production deployment.
      </p>
    </section>

    <section>
      <h2>Introduction</h2>
      <p>
This specification defines a cryptographic suite for the purpose of creating,
and verifying proofs for ECDSA signatures in conformance with the
Data Integrity [[VC-DATA-INTEGRITY]] specification. ECDSA signatures are
specified in [[FIPS-186-5]] with elliptic curves P-256 and P-384 specified in
[[NIST-SP-800-186]]. [[FIPS-186-5]] includes the <em>deterministic</em> ECDSA
algorithm which is also specified in [[RFC6979]].
      </p>
      <p>
This specification uses either the RDF Dataset Canonicalization Algorithm
[[RDF-CANON]] or the JSON Canonicalization Scheme [[RFC8785]] to transform the
input document into its canonical form. It uses one of two mechanisms to digest
and sign: SHA-256 [[RFC6234]] as the message digest algorithm and ECDSA with
Curve P-256 as the signature algorithm, or SHA-384 [[RFC6234]] as the message
digest algorithm and ECDSA with Curve P-384 as the signature algorithm.
      </p>
      <p class="note">
The elliptic curves P-256 and P-384 of [[NIST-SP-800-186]] are referred to as
<em>secp256r1</em> and <em>secp384r1</em> respectively in [[SECG2]]. In
addition, this notation is sometimes used in ECDSA software libraries.
      </p>

      <section id="terminology">
        <h3>Terminology</h3>

        <div data-include="https://w3c.github.io/vc-data-integrity/terms.html"></div>

      </section>

      <section id="conformance">
        <p>
A <dfn>conforming proof</dfn> is any concrete expression of the data model
that complies with the normative statements in this specification. Specifically,
all relevant normative statements in Sections
<a href="#data-model"></a> and <a href="#algorithms"></a>
of this document MUST be enforced.
        </p>

        <p>
A <dfn class="lint-ignore">conforming processor</dfn> is any algorithm realized
as software and/or hardware that generates or consumes a
<a>conforming proof</a>. Conforming processors MUST produce errors when
non-conforming documents are consumed.
        </p>
        <p>
This document also contains examples that contain JSON and JSON-LD content. Some
of these examples contain characters that are invalid JSON, such as inline
comments (`//`) and the use of ellipsis (`...`) to denote
information that adds little value to the example. Implementers are cautioned to
remove this content if they desire to use the information as valid JSON or
JSON-LD.
        </p>
      </section>

    </section>

    <section>
      <h2>Data Model</h2>

      <p>
The following sections outline the data model that is used by this specification
to express verification methods, such as cryptographic public keys, and
data integrity proofs, such as digital signatures.
      </p>

      <section>
        <h3>Verification Methods</h3>

        <p>
These verification methods are used to verify Data Integrity Proofs
[[VC-DATA-INTEGRITY]] produced using Elliptic Curve cryptographic key material
that is compliant with [[FIPS-186-5]]. The encoding formats for these key types
are provided in this section. Lossless cryptographic key transformation
processes that result in equivalent cryptographic key material MAY be used
during the processing of digital signatures.
        </p>

        <section>
          <h4>Multikey</h4>

          <p>
The <a data-cite="VC-DATA-INTEGRITY#multikey">Multikey format</a>, as defined in
[[VC-DATA-INTEGRITY]], is used to express public keys for the cryptographic
suites defined in this specification.
          </p>

          <p>
The `publicKeyMultibase` property represents a Multibase-encoded Multikey
expression of a P-256 or P-384 public key. The encoding of a P-256 public key is
the two-byte prefix `0x8024` (the varint expression of `0x1200`) followed
by the 33-byte compressed public key data.
The 35-byte value is then encoded using base58-btc (`z`) as the prefix. The
encoding of a P-384 public key is the two-byte prefix `0x8124` (the varint
expression of `0x1201`) followed by the 49-byte compressed public key data.
The 51-byte value is then encoded using base58-btc (`z`) as the prefix. Any
other encodings MUST NOT be allowed.
          </p>

          <p class="advisement">
Developers are advised to not accidentally publish a representation of a private
key. Implementations of this specification will raise errors in the event of a
[[MULTICODEC]] value other than `0x1200` or `0x1201` being used in a
`publicKeyMultibase` value.
          </p>

          <pre class="example nohighlight"
            title="An P-256 public key encoded as a Multikey">
{
  "id": "https://example.com/issuer/123#key-0",
  "type": "Multikey",
  "controller": "https://example.com/issuer/123",
  "publicKeyMultibase": "zDnaerx9CtbPJ1q36T5Ln5wYt3MQYeGRG5ehnPAmxcf5mDZpv"
}
          </pre>

          <pre class="example nohighlight"
            title="An P-384 public key encoded as a Multikey">
{
  "id": "https://example.com/issuer/123#key-0",
  "type": "Multikey",
  "controller": "https://example.com/issuer/123",
  "publicKeyMultibase": "z82LkvCwHNreneWpsgPEbV3gu1C6NFJEBg4srfJ5gdxEsMGRJ
    Uz2sG9FE42shbn2xkZJh54"
}
          </pre>

          <pre class="example nohighlight" title="Two public keys (P-256 and P-384)
            encoded as Multikeys in a controller document">
{
  "@context": [
    "https://www.w3.org/ns/did/v1",
    "https://w3id.org/security/data-integrity/v1"
  ],
  "id": "did:example:123",
  "verificationMethod": [{
    "id": "https://example.com/issuer/123#key-1",
    "type": "Multikey",
    "controller": "https://example.com/issuer/123",
    "publicKeyMultibase": "zDnaerx9CtbPJ1q36T5Ln5wYt3MQYeGRG5ehnPAmxcf5mDZpv"
  }, {
    "id": "https://example.com/issuer/123#key-2",
    "type": "Multikey",
    "controller": "https://example.com/issuer/123",
    "publicKeyMultibase": "z82LkvCwHNreneWpsgPEbV3gu1C6NFJEBg4srfJ5gdxEsMGRJ
      Uz2sG9FE42shbn2xkZJh54"
  }],
  "authentication": [
    "did:example:123#key-1"
  ],
  "assertionMethod": [
    "did:example:123#key-2"
  ],
  "capabilityDelegation": [
    "did:example:123#key-2"
  ],
  "capabilityInvocation": [
    "did:example:123#key-2"
  ]
}
          </pre>
        </section>

      </section>

      <section>
        <h3>Proof Representations</h3>

        <p>
This suite relies on detached digital signatures represented using [[MULTIBASE]]
and [[MULTICODEC]].
        </p>

        <section>
          <h4>DataIntegrityProof</h4>

          <p>
The `verificationMethod` property of the proof MUST be a URL.
Dereferencing the `verificationMethod` MUST result in an object
containing a `type` property with the value set to
`Multikey`.
          </p>

          <p>
The `type` property of the proof MUST be `DataIntegrityProof`.
          </p>
          <p>
The `cryptosuite` property of the proof MUST be `ecdsa-rdfc-2019` or `ecdsa-jcs-2019`.
          </p>
          <p>
The `created` property of the proof MUST be an [[XMLSCHEMA11-2]]
formatted date string.
          </p>
          <p>
The `proofPurpose` property of the proof MUST be a string, and MUST
match the verification relationship expressed by the verification method
`controller`.
          </p>
          <p>
The `proofValue` property of the proof MUST be an ECDSA or deterministic ECDSA
signature produced according to [[FIPS-186-5]] using the curves and hashes as
specified in section <a href="#algorithms"></a>, encoded according to section 7
of [[RFC4754]] (sometimes referred to as the IEEE P1363 format), and serialized
according to [[MULTIBASE]] using the base58-btc base encoding.
          </p>

          <pre class="example nohighlight"
            title="An ECDSA P-256 digital signature expressed as a
              DataIntegrityProof">
{
  "@context": [
    {"title": "https://schema.org/title"},
    "https://w3id.org/security/data-integrity/v1"
  ],
  "title": "Hello world!",
  "proof": {
    "type": "DataIntegrityProof",
    "cryptosuite": "ecdsa-2019",
    "created": "2020-11-05T19:23:24Z",
    "verificationMethod": "https://example.com/issuer/123#key-2",
    "proofPurpose": "assertionMethod",
    "proofValue": "z4oey5q2M3XKaxup3tmzN4DRFTLVqpLMweBrSxMY2xHX5XTYVQeVbY8nQA
      VHMrXFkXJpmEcqdoDwLWxaqA3Q1geV6"
  }
}
          </pre>

        </section>
      </section>
    </section>

    <section>
      <h2>Algorithms</h2>

      <p>
The following section describes multiple Data Integrity cryptographic suites
that utilize the Elliptic Curve Digital Signature Algorithm (ECDSA) [[FIPS-186-5]].
      </p>

      <section>
        <h3>ecdsa-rdfc-2019</h3>

        <p>
The `ecdsa-rdfc-2019` cryptographic suite takes an input document, canonicalizes
the document using the Universal RDF Dataset Canonicalization Algorithm
[[RDF-CANON]], and then cryptographically hashes and signs the output
resulting in the production of a data integrity proof. The algorithms in this
section also include the verification of such a data integrity proof.
        </p>

        <p class="advisement">
When the RDF Dataset Canonicalization Algorithm [[RDF-CANON]] is used,
implementations of that algorithm MUST detect
<a data-cite="RDF-CANON#dataset-poisoning">dataset poisoning</a>
by default, and abort processing upon detection.
        </p>

        <section>
          <h4>Add Proof (ecdsa-rdfc-2019)</h4>

          <p>
To generate a proof, the algorithm in
<a href="https://www.w3.org/TR/vc-data-integrity/#add-proof">
Section 4.1: Add Proof</a> in the Data Integrity
[[VC-DATA-INTEGRITY]] specification MUST be executed.
For that algorithm, the cryptographic suite specific
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-transformation-algorithm">
transformation algorithm</a> is defined in Section
<a href="#transformation-ecdsa-rdfc-2019"></a>, the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-hashing-algorithm">
hashing algorithm</a> is defined in Section <a href="#hashing-ecdsa-rdfc-2019"></a>,
and the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-proof-serialization-algorithm">
proof serialization algorithm</a> is defined in Section
<a href="#proof-serialization-ecdsa-rdfc-2019"></a>.
          </p>
        </section>

        <section>
          <h4>Verify Proof (ecdsa-rdfc-2019)</h4>

          <p>
To verify a proof, the algorithm in
<a href="https://www.w3.org/TR/vc-data-integrity/#verify-proof">
Section 4.2: Verify Proof</a> in the Data Integrity
[[VC-DATA-INTEGRITY]] specification MUST be executed.
For that algorithm, the cryptographic suite specific
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-transformation-algorithm">
transformation algorithm</a> is defined in Section
<a href="#transformation-ecdsa-rdfc-2019"></a>, the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-hashing-algorithm">
hashing algorithm</a> is defined in Section <a href="#hashing-ecdsa-rdfc-2019"></a>,
and the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-proof-serialization-algorithm">
proof verification algorithm</a> is defined in Section
<a href="#proof-verification-ecdsa-rdfc-2019"></a>.
          </p>
        </section>

        <section>
          <h4>Transformation (ecdsa-rdfc-2019)</h4>

          <p>
The following algorithm specifies how to transform an unsecured input document
into a transformed document that is ready to be provided as input to the
hashing algorithm in Section <a href="#hashing-ecdsa-rdfc-2019"></a>.
          </p>

          <p>
Required inputs to this algorithm are an
<a data-cite="vc-data-integrity#dfn-unsecured-data-document">
unsecured data document</a> (<var>unsecuredDocument</var>) and
transformation options (<var>options</var>). The
transformation options MUST contain a type identifier for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>) and a cryptosuite
identifier (<var>cryptosuite</var>). A <em>transformed data document</em> is
produced as output. Whenever this algorithm encodes strings, it MUST use UTF-8
encoding.
          </p>

          <ol class="algorithm">
            <li>
If <var>options</var>.<var>type</var> is not set to the string
`DataIntegrityProof` and <var>options</var>.<var>cryptosuite</var> is not
set to the string `ecdsa-rdfc-2019` then a `PROOF_TRANSFORMATION_ERROR` MUST be
raised.
            </li>
            <li>
Let <var>canonicalDocument</var> be the result of applying the
Universal RDF Dataset Canonicalization Algorithm
[[RDF-CANON]] to the <var>unsecuredDocument</var>.
            </li>
            <li>
Set <var>output</var> to the value of <var>canonicalDocument</var>.
            </li>
            <li>
Return <var>canonicalDocument</var> as the <em>transformed data document</em>.
            </li>
          </ol>
        </section>

        <section>
          <h4>Hashing (ecdsa-rdfc-2019)</h4>

          <p>
The following algorithm specifies how to cryptographically hash a
<em>transformed data document</em> and <em>proof configuration</em>
into cryptographic hash data that is ready to be provided as input to the
algorithms in Section <a href="#proof-serialization-ecdsa-rdfc-2019"></a> or
Section <a href="#proof-verification-ecdsa-rdfc-2019"></a>. One must use the hash
algorithm appropriate in security level to the curve used, i.e., for curve
P-256 one uses SHA-256 and for curve P-384 one uses SHA-384.
          </p>

          <p>
The required inputs to this algorithm are a <em>transformed data document</em>
(<var>transformedDocument</var>) and <em>canonical proof configuration</em>
(<var>canonicalProofConfig</var>). A single <em>hash data</em> value represented as
series of bytes is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>transformedDocumentHash</var> be the result of applying the
SHA-256 (SHA-2 with 256-bit output) or SHA-384 (SHA-2 with 384-bit output)
cryptographic hashing algorithm [[RFC6234]] to the
respective curve P-256 or curve P-384 <var>transformedDocument</var>.
Respective <var>transformedDocumentHash</var> will be exactly 32 or 48 bytes
in size.
            </li>
            <li>
Let <var>proofConfigHash</var> be the result of applying the
SHA-256 (SHA-2 with 256-bit output) or SHA-384 (SHA-2 with 384-bit output)
cryptographic hashing algorithm [[RFC6234]] to the respective curve P-256 or curve P-384
<var>canonicalProofConfig</var>. Respective <var>proofConfigHash</var>
will be exactly 32 or 48 bytes in size.
            </li>
            <li>
Let <var>hashData</var> be the result of joining <var>proofConfigHash</var> (the
first hash) with <var>transformedDocumentHash</var> (the second hash).
            </li>
            <li>
Return <var>hashData</var> as the <em>hash data</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Proof Configuration (ecdsa-rdfc-2019)</h4>

          <p>
The following algorithm specifies how to generate a
<em>proof configuration</em> from a set of <em>proof options</em>
that is used as input to the <a href="#hashing-ecdsa-rdfc-2019">proof hashing algorithm</a>.
          </p>

          <p>
The required inputs to this algorithm are <em>proof options</em>
(<var>options</var>). The <em>proof options</em> MUST contain a type identifier
for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>) and MUST contain a cryptosuite
identifier (<var>cryptosuite</var>). A <em>proof configuration</em>
object is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>proofConfig</var> be an empty object.
            </li>
            <li>
Set <var>proofConfig</var>.<var>type</var> to
<var>options</var>.<var>type</var>.
            </li>
            <li>
If <var>options</var>.<var>cryptosuite</var> is set, set
<var>proofConfig</var>.<var>cryptosuite</var> to its value.
            </li>
            <li>
If <var>options</var>.<var>type</var> is not set to `DataIntegrityProof` and
<var>proofConfig</var>.<var>cryptosuite</var> is not set to `ecdsa-rdfc-2019`, an
`INVALID_PROOF_CONFIGURATION` error MUST be raised.
            </li>
            <li>
Set <var>proofConfig</var>.<var>created</var> to
<var>options</var>.<var>created</var>. If the value is not a valid
[[XMLSCHEMA11-2]] datetime, an `INVALID_PROOF_DATETIME` error MUST be raised.
            </li>
            <li>
Set <var>proofConfig</var>.<var>verificationMethod</var> to
<var>options</var>.<var>verificationMethod</var>.
            </li>
            <li>
Set <var>proofConfig</var>.<var>proofPurpose</var> to
<var>options</var>.<var>proofPurpose</var>.
            </li>
            <li>
Set <var>proofConfig</var>.<var>@context</var> to
<var>unsecuredDocument</var>.<var>@context</var>.
            </li>
            <li>
Let <var>canonicalProofConfig</var> be the result of applying the
Universal RDF Dataset Canonicalization Algorithm
[[RDF-CANON]] to the <var>proofConfig</var>.
            </li>
            <li>
Return <var>canonicalProofConfig</var>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Proof Serialization (ecdsa-rdfc-2019)</h4>

          <p>
The following algorithm specifies how to serialize a digital signature from
a set of cryptographic hash data. This
algorithm is designed to be used in conjunction with the algorithms defined
in the Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Algorithms</a>. Required inputs are
cryptographic hash data (<var>hashData</var>) and
<em>proof options</em> (<var>options</var>). The
<em>proof options</em> MUST contain a type identifier for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>) and MAY contain a cryptosuite
identifier (<var>cryptosuite</var>). A single <em>digital proof</em> value
represented as series of bytes is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>privateKeyBytes</var> be the result of retrieving the
private key bytes associated with the
<var>options</var>.<var>verificationMethod</var> value as described in the
Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Retrieving Cryptographic Material</a>.
            </li>
            <li>
Let <var>proofBytes</var> be the result of applying the Elliptic Curve Digital
Signature Algorithm (ECDSA) [[FIPS-186-5]], with <var>hashData</var> as the data
to be signed using the private key specified by <var>privateKeyBytes</var>.
<var>proofBytes</var> will be exactly 64 bytes in size for a P-256 key, and
96 bytes in size for a P-384 key.
            </li>
            <li>
Return <var>proofBytes</var> as the <em>digital proof</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Proof Verification (ecdsa-rdfc-2019)</h4>

          <p>
The following algorithm specifies how to verify a digital signature from
a set of cryptographic hash data. This
algorithm is designed to be used in conjunction with the algorithms defined
in the Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Algorithms</a>. Required inputs are
cryptographic hash data (<var>hashData</var>),
a digital signature (<var>proofBytes</var>) and
proof options (<var>options</var>). A <em>verification result</em>
represented as a boolean value is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>publicKeyBytes</var> be the result of retrieving the
public key bytes associated with the
<var>options</var>.<var>verificationMethod</var> value as described in the
Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Retrieving Cryptographic Material</a>.
            </li>
            <li>
Let <var>verificationResult</var> be the result of applying the verification
algorithm Elliptic Curve Digital Signature Algorithm (ECDSA) [[FIPS-186-5]],
with <var>hashData</var> as the data to be verified against the
<var>proofBytes</var> using the public key specified by
<var>publicKeyBytes</var>.
            </li>
            <li>
Return <var>verificationResult</var> as the <em>verification result</em>.
            </li>
          </ol>

        </section>
      </section>
      <section>
        <h3>ecdsa-jcs-2019</h3>

        <p>
The `ecdsa-jcs-2019` cryptographic suite takes an input document, canonicalizes
the document using the JSON Canonicalization Scheme [[RFC8785]], and then
cryptographically hashes and signs the output
resulting in the production of a data integrity proof. The algorithms in this
section also include the verification of such a data integrity proof.
        </p>

        <section>
          <h4>Add Proof (ecdsa-jcs-2019)</h4>

          <p>
To generate a proof, the algorithm in
<a href="https://www.w3.org/TR/vc-data-integrity/#add-proof">
Section 4.1: Add Proof</a> of the Data Integrity
[[VC-DATA-INTEGRITY]] specification MUST be executed.
For that algorithm, the cryptographic suite-specific
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-transformation-algorithm">
transformation algorithm</a> is defined in Section
<a href="#transformation-ecdsa-jcs-2019"></a>, the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-hashing-algorithm">
hashing algorithm</a> is defined in Section <a href="#hashing-ecdsa-jcs-2019"></a>,
and the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-proof-serialization-algorithm">
proof serialization algorithm</a> is defined in Section
<a href="#proof-serialization-ecdsa-jcs-2019"></a>.
          </p>
        </section>

        <section>
          <h4>Verify Proof (ecdsa-jcs-2019)</h4>

          <p>
To verify a proof, the algorithm in
<a href="https://www.w3.org/TR/vc-data-integrity/#verify-proof">
Section 4.2: Verify Proof</a> of the Data Integrity
[[VC-DATA-INTEGRITY]] specification MUST be executed.
For that algorithm, the cryptographic suite-specific
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-transformation-algorithm">
transformation algorithm</a> is defined in Section
<a href="#transformation-ecdsa-jcs-2019"></a>, the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-hashing-algorithm">
hashing algorithm</a> is defined in Section <a href="#hashing-ecdsa-jcs-2019"></a>,
and the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-proof-serialization-algorithm">
proof verification algorithm</a> is defined in Section
<a href="#proof-verification-ecdsa-jcs-2019"></a>.
          </p>
        </section>

        <section>
          <h4>Transformation (ecdsa-jcs-2019)</h4>

          <p>
The following algorithm specifies how to transform an unsecured input document
into a transformed document that is ready to be provided as input to the
hashing algorithm in Section <a href="#hashing-ecdsa-jcs-2019"></a>.
          </p>

          <p>
Required inputs to this algorithm are an
<a data-cite="vc-data-integrity#dfn-unsecured-data-document">
unsecured data document</a> (<var>unsecuredDocument</var>) and
transformation options (<var>options</var>). The
transformation options MUST contain a type identifier for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>) and a cryptosuite
identifier (<var>cryptosuite</var>). A <em>transformed data document</em> is
produced as output. Whenever this algorithm encodes strings, it MUST use UTF-8
encoding.
          </p>

          <ol class="algorithm">
            <li>
If <var>options</var>.<var>type</var> is not set to the string
`DataIntegrityProof` and <var>options</var>.<var>cryptosuite</var> is not
set to the string `ecdsa-jcs-2019`, then a `PROOF_TRANSFORMATION_ERROR` MUST be
raised.
            </li>
            <li>
Let <var>canonicalDocument</var> be the result of applying the
JSON Canonicalization Scheme [[RFC8785]] to the <var>unsecuredDocument</var>.
            </li>
            <li>
Set <var>output</var> to the value of <var>canonicalDocument</var>.
            </li>
            <li>
Return <var>canonicalDocument</var> as the <em>transformed data document</em>.
            </li>
          </ol>
        </section>

        <section>
          <h4>Hashing (ecdsa-jcs-2019)</h4>

          <p>
The following algorithm specifies how to cryptographically hash a
<em>transformed data document</em> and <em>proof configuration</em>
into cryptographic hash data that is ready to be provided as input to the
algorithms in Section <a href="#proof-serialization-ecdsa-jcs-2019"></a> or
Section <a href="#proof-verification-ecdsa-jcs-2019"></a>. One must use the
hash algorithm appropriate in security level to the curve used, i.e., for curve
P-256 one uses SHA-256, and for curve P-384 one uses SHA-384.
          </p>

          <p>
The required inputs to this algorithm are a <em>transformed data document</em>
(<var>transformedDocument</var>) and a <em>canonical proof configuration</em>
(<var>canonicalProofConfig</var>). A single <em>hash data</em> value represented as
series of bytes is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>transformedDocumentHash</var> be the result of applying the SHA-256
(SHA-2 with 256-bit output) or SHA-384 (SHA-2 with 384-bit output)
cryptographic hashing algorithm [[RFC6234]] to the
respective curve P-256 or curve P-384 <var>transformedDocument</var>.
Respective <var>transformedDocumentHash</var> will be exactly 32 or 48 bytes
in size.
            </li>
            <li>
Let <var>proofConfigHash</var> be the result of applying the SHA-256
(SHA-2 with 256-bit output) or SHA-384 (SHA-2 with 384-bit output)
cryptographic hashing algorithm [[RFC6234]] to the
respective curve P-256 or curve P-384 <var>canonicalProofConfig</var>.
Respective <var>proofConfigHash</var> will be exactly 32 or 48 bytes in size.
            </li>
            <li>
Let <var>hashData</var> be the result of concatenating <var>proofConfigHash</var> (the
first hash) followed by <var>transformedDocumentHash</var> (the second hash).
            </li>
            <li>
Return <var>hashData</var> as the <em>hash data</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Proof Configuration (ecdsa-jcs-2019)</h4>

          <p>
The following algorithm specifies how to generate a
<em>proof configuration</em> from a set of <em>proof options</em>
that is used as input to the <a href="#hashing-ecdsa-jcs-2019">proof hashing algorithm</a>.
          </p>

          <p>
The required inputs to this algorithm are <em>proof options</em>
(<var>options</var>). The <em>proof options</em> MUST contain a type identifier
for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>) and MUST contain a cryptosuite
identifier (<var>cryptosuite</var>). A <em>proof configuration</em>
object is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>proofConfig</var> be an empty object.
            </li>
            <li>
Set <var>proofConfig</var>.<var>type</var> to
<var>options</var>.<var>type</var>.
            </li>
            <li>
If <var>options</var>.<var>cryptosuite</var> is set, set
<var>proofConfig</var>.<var>cryptosuite</var> to its value.
            </li>
            <li>
If <var>options</var>.<var>type</var> is not set to `DataIntegrityProof` and
<var>proofConfig</var>.<var>cryptosuite</var> is not set to `ecdsa-jcs-2019`, an
`INVALID_PROOF_CONFIGURATION` error MUST be raised.
            </li>
            <li>
Set <var>proofConfig</var>.<var>created</var> to
<var>options</var>.<var>created</var>. If the value is not a valid
[[XMLSCHEMA11-2]] datetime, an `INVALID_PROOF_DATETIME` error MUST be raised.
            </li>
            <li>
Set <var>proofConfig</var>.<var>verificationMethod</var> to
<var>options</var>.<var>verificationMethod</var>.
            </li>
            <li>
Set <var>proofConfig</var>.<var>proofPurpose</var> to
<var>options</var>.<var>proofPurpose</var>.
            </li>
            <li>
Let <var>canonicalProofConfig</var> be the result of applying the
JSON Canonicalization Scheme [[RFC8785]] to the <var>proofConfig</var>.
            </li>
            <li>
Return <var>canonicalProofConfig</var>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Proof Serialization (ecdsa-jcs-2019)</h4>

          <p>
The following algorithm specifies how to serialize a digital signature from
a set of cryptographic hash data. This
algorithm is designed to be used in conjunction with the algorithms defined
in the Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Algorithms</a>. Required inputs are
cryptographic hash data (<var>hashData</var>) and
<em>proof options</em> (<var>options</var>). The
<em>proof options</em> MUST contain a type identifier for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>) and MAY contain a cryptosuite
identifier (<var>cryptosuite</var>). A single <em>digital proof</em> value
represented as series of bytes is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>privateKeyBytes</var> be the result of retrieving the
private key bytes associated with the
<var>options</var>.<var>verificationMethod</var> value as described in the
Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Retrieving Cryptographic Material</a>.
            </li>
            <li>
Let <var>proofBytes</var> be the result of applying the Elliptic Curve Digital
Signature Algorithm (ECDSA) [[FIPS-186-5]], with <var>hashData</var> as the data
to be signed using the private key specified by <var>privateKeyBytes</var>.
<var>proofBytes</var> will be exactly 64 bytes in size for a P-256 key, and
96 bytes in size for a P-384 key.
            </li>
            <li>
Return <var>proofBytes</var> as the <em>digital proof</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Proof Verification (ecdsa-jcs-2019)</h4>

          <p>
The following algorithm specifies how to verify a digital signature from
a set of cryptographic hash data. This
algorithm is designed to be used in conjunction with the algorithms defined
in the Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Algorithms</a>. Required inputs are
cryptographic hash data (<var>hashData</var>),
a digital signature (<var>proofBytes</var>), and
proof options (<var>options</var>). A <em>verification result</em>
represented as a boolean value is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>publicKeyBytes</var> be the result of retrieving the
public key bytes associated with the
<var>options</var>.<var>verificationMethod</var> value as described in the
Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Retrieving Cryptographic Material</a>.
            </li>
            <li>
Let <var>verificationResult</var> be the result of applying the verification
algorithm, Elliptic Curve Digital Signature Algorithm (ECDSA) [[FIPS-186-5]],
with <var>hashData</var> as the data to be verified against the
<var>proofBytes</var> using the public key specified by
<var>publicKeyBytes</var>.
            </li>
            <li>
Return <var>verificationResult</var> as the <em>verification result</em>.
            </li>
          </ol>

        </section>
      </section>

      <section>
        <h4>Selective Disclosure Functions</h4>

        <p class="issue" title="(AT RISK) Pending implementation feedback and security reviews.">
The Working Group is seeking implementer feedback on these generalized selective
disclosure functions as well as horizonal security review on the features from
parties at W3C and IETF. Those reviews might result in significant changes to
these functions, migration of these functions to the core Data Integrity
specification (for use by other cryptographic suites), or the removal of the
algorithm from the specification during the Candidate Recommendation phase.
        </p>

        <p>
The following section contains a set of functions that are used throughout
cryptographic suites that perform selective disclosure.
        </p>

        <section>
          <h4>labelReplacementCanonicalizeNQuads</h4>

          <p>
The following algorithm canonicalizes an array of N-Quad strings and replaces
any blank node identifiers in the canonicalized result using a label map
factory function, <var>labelMapFactoryFunction</var>. The required inputs are
an array of N-Quad strings (<var>nquads</var>), and a label map factory function
(<var>labelMapFactoryFunction</var>). Any custom options can also be passed. An
N-Quads representation of the <em>canonicalNQuads</em> as an array of N-Quad
strings, with the replaced blank node labels, and a map from the old blank node
IDs to the new blank node IDs, <em>labelMap</em>, is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Run the RDF Dataset Canonicalization Algorithm [[RDF-CANON]] on
the joined <var>nquads</var>, passing any custom options, and as output,
get the canonicalized dataset, which includes a canonical bnode identifier
map, <var>canonicalIdMap</var>.
            </li>
            <li>
Pass <var>canonicalIdMap</var> to <var>labelMapFactoryFunction</var> to produce
a new bnode identifier map, <em>labelMap</em>.
            </li>
            <li>
Use the canonicalized dataset and <em>labelMap</em> to produce the canonical
N-Quads representation as an array of N-Quad strings, <em>canonicalNQuads</em>.
            </li>
            <li>
Return an object containing <em>labelMap</em> and <em>canonicalNQuads</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>labelReplacementCanonicalizeJsonLd</h4>

          <p>
The following algorithm canonicalizes a JSON-LD document and replaces any blank
node identifiers in the canonicalized result using a label map factory
function, <var>labelMapFactoryFunction</var>. The required inputs are a JSON-LD
document (<var>document</var>) and a label map factory function
(<var>labelMapFactoryFunction</var>). Additional custom options (such as
a document loader) can also be passed. An N-Quads representation of the
<em>canonicalNQuads</em> as an array of N-Quad strings, with the replaced
blank node labels, and a map from the old blank node IDs to the new blank node
IDs, <em>labelMap</em>, is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Deserialize the JSON-LD document to RDF, <var>rdf</var>, using the <a
href="https://www.w3.org/TR/json-ld11-api/#deserialize-json-ld-to-rdf-algorithm">
Deserialize JSON-LD to RDF algorithm</a>, passing any custom options (such
as a document loader).
            </li>
            <li>
Serialize <var>rdf</var> to an array of N-Quad strings, <var>nquads</var>.
            </li>
            <li>
Return the result of calling the algorithm in Section
<a href="#labelreplacementcanonicalizenquads"></a>, passing <var>nquads</var>,
<var>labelMapFactoryFunction</var>, and any custom options.
            </li>
          </ol>

        </section>

        <section>
          <h4>createLabelMapFunction</h4>

          <p>
The following algorithm creates a label map factory function that uses an
input label map to replace canonical blank node identifiers with another
value. The required input is a label map, <var>labelMap</var>. A
function, <em>labelMapFactoryFunction</em>, is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Create a function, <var>labelMapFactoryFunction</var>, with one required input
(a canonical node identifier map, <var>canonicalIdMap</var>), that will
return a blank node identifier map, <em>bnodeIdMap</em>, as output. Set the
function's implementation to:
              <ol class="algorithm">
                <li>
Generate a new empty bnode identifier map, <em>bnodeIdMap</em>.
                </li>
                <li>
For each map entry, <em>entry</em>, in <var>canonicalIdMap</var>:
                  <ol class="algorithm">
                    <li>
Use the canonical identifier from the value in <em>entry</em> as a key
in <var>labelMap</var> to get the new label, <em>newLabel</em>.
                    </li>
                    <li>
Add a new entry, <var>newEntry</var>, to <em>bnodeIdMap</em> using the key
from <em>entry</em> and <em>newLabel</em> as the value.
                    </li>
                  </ol>
                </li>
                <li>
Return <em>bnodeIdMap</em>.
                </li>
              </ol>
            </li>
            <li>
Return <var>labelMapFactoryFunction</var>.
            </li>
          </ol>

        </section>

        <section>
          <h4>createHmacIdLabelMapFunction</h4>

          <p>
The following algorithm creates a label map factory function that uses an
HMAC to replace canonical blank node identifiers with their encoded HMAC
digests. The required input is an HMAC (previously initialized with a
secret key), <var>HMAC</var>. A function, <em>labelMapFactoryFunction</em>, is
produced as output.
          </p>

          <ol class="algorithm">
            <li>
Create a function, <var>labelMapFactoryFunction</var>, with one required input
(a canonical node identifier map, <var>canonicalIdMap</var>), that will
return a blank node identifier map, <em>bnodeIdMap</em>, as output. Set the
function's implementation to:
              <ol class="algorithm">
                <li>
Generate a new empty bnode identifier map, <em>bnodeIdMap</em>.
                </li>
                <li>
For each map entry, <em>entry</em>, in <var>canonicalIdMap</var>:
                  <ol class="algorithm">
                    <li>
HMAC the canonical identifier from the value in <em>entry</em> to get an HMAC
digest, <em>digest</em>.
                    </li>
                    <li>
Generate a new string value, <em>b64urlDigest</em>, and initialize it to "u"
followed by appending a base64url-no-pad encoded version of the <em>digest</em>
value.
                    </li>
                    <li>
Add a new entry, <var>newEntry</var>, to <em>bnodeIdMap</em> using the key
from <em>entry</em> and <em>b64urlDigest</em> as the value.
                    </li>
                  </ol>
                </li>
                <li>
Return <em>bnodeIdMap</em>.
                </li>
              </ol>
            </li>
            <li>
Return <var>labelMapFactoryFunction</var>.
            </li>
          </ol>

          <p class="note" title="Other algorithms are possible">
A different primitive could be created that returned a label map factory
function that would instead sort the resulting HMAC digests and assign labels
in the produced label map using a prefix and integers based on their sorted
order. This primitive might be useful for selective disclosure schemes,
such as BBS, that favor unlinkability over minimizing unrevealed data leakage.
          </p>

        </section>

        <section>
          <h4>skolemizeNQuads</h4>

          <p>
The following algorithm replaces all blank node identifiers in an array of
N-Quad strings with custom scheme URNs. The required inputs are an array of
N-Quad strings (<var>inputNQuads</var>) and a URN scheme (<var>urnScheme</var>).
An array of N-Quad strings, <em>skolemizedNQuads</em>, is produced as output.
This operation is intended to be reversible through the use of the algorithm in
Section <a href="#deskolemizenquads"></a>.
          </p>

          <ol class="algorithm">
            <li>
Create a new array of N-Quad strings, <em>skolemizedNQuads</em>.
            </li>
            <li>
For each N-Quad string, <em>s1</em>, in <var>inputNQuads</var>:
              <ol class="algorithm">
                <li>
Create a new string, <em>s2</em>, that is a copy of <em>s1</em> replacing any
occurrence of a blank node identifier with a URN ("urn:"), plus the input
custom scheme (<var>urnScheme</var>), plus a colon (":"), and
the value of the blank node identifier. For example, a regular expression
of a similar form to the following would achieve the desired result:
<code>s1.replace(/(_:([^\s]+))/g, '&lt;urn:custom-scheme:$2>')</code>.
                </li>
                <li>
Append <em>s2</em> to <em>skolemizedNQuads</em>.
                </li>
              </ol>
            </li>
            <li>
Return <em>skolemizedNQuads</em>.
            </li>
          </ol>
        </section>

        <section>
          <h4>deskolemizeNQuads</h4>

          <p>
The following algorithm replaces all custom scheme URNs in an array of
N-Quad statements with a blank node identifier. The required inputs are an array
of N-Quad strings (<var>inputNQuads</var>) and a URN scheme
(<var>urnScheme</var>). An array of N-Quad strings,
<em>deskolemizedNquads</em>, is produced as output. This operation is intended
to reverse use of the algorithm in Section <a href="#deskolemizenquads"></a>.
          </p>

          <ol class="algorithm">
            <li>
Create a new array of N-Quad strings, <em>deskolemizedNQuads</em>.
            </li>
            <li>
For each N-Quad string, <em>s1</em>, in <var>inputNQuads</var>:
              <ol class="algorithm">
                <li>
Create a new string, <em>s2</em>, that is a copy of <em>s1</em> replacing any
occurrence of a URN ("urn:"), plus the input
custom scheme (<var>urnScheme</var>), plus a colon (":"), and
the value of the blank node identifier with a blank node prefix ("_:"), plus
the value of the blank node identifier. For example, a regular expression
of a similar form to the following would achieve the desired result:
<code>s1.replace(/(&lt;urn:custom-scheme:([^>]+)>)/g, '_:$2').</code>.
                </li>
                <li>
Append <em>s2</em> to <em>deskolemizedNQuads</em>.
                </li>
              </ol>
            </li>
            <li>
Return <em>deskolemizedNQuads</em>.
            </li>
          </ol>
        </section>

        <section>
          <h4>skolemizeExpandedJsonLd</h4>

          <p>
The following algorithm replaces all blank node identifiers in an expanded
JSON-LD document with custom-scheme URNs. The required inputs are an expanded
JSON-LD document (<var>expanded</var>), a custom URN scheme
(<var>urnScheme</var>), a UUID string or other comparably random string
(<var>random</var>), and reference to a shared integer (<var>count</var>).
Any additional custom options (such as a document loader) can also be passed.
It produces the expanded form of the skolemized JSON-LD document
(<var>skolemizedExpandedDocument</var> as output. The skolemization used in this
operation is intended to be reversible through the use of the algorithm in
Section <a href="#todeskolemizednquads"></a>.
          </p>

          <ol class="algorithm">
            <li>
Initialize <var>skomelizedExpanded</var> to an empty array.
            </li>
            <li>
For each <var>element</var> in <var>expanded</var>:
              <ol class="algorithm">
                <li>
If either <var>element</var> is not an object or it contains the key
<em>@value</em>, append a copy of <var>element</var> to
<var>skolemizedExpandedDocument</var> and continue to the next
<var>element</var>.
                </li>
                <li>
Otherwise, initialize <var>skolemizedNode</var> to an object, and for
each <em>property</em> and <em>value</em> in <var>element</var>:
                  <ol class="algorithm">
                    <li>
If <em>value</em> is an array, set the value of <em>property</em> in
<var>skolemizedNode</var> to the result of calling this algorithm
recursively passing <em>value</em> for <var>expanded</var> and keeping the
other parameters the same.
                    </li>
                    <li>
Otherwise, set the value of <em>property</em> in <var>skolemizedNode</var> to
the first element in the array result of calling this algorithm recursively
passing an array with <em>value</em> as its only element for <var>expanded</var>
and keeping the other parameters the same.
                    </li>
                  </ol>
                </li>
                <li>
If <var>skolemizedNode</var> has no <em>@id</em> property, set the value of
the <em>@id</em> property in <var>skolemizedNode</var> to the concatenation
of "urn:", <var>urnScheme</var>, <var>random</var>, "_" and the value of
<var>count</var>, incrementing the value of <var>count</var> afterwards.
                </li>
                <li>
Otherwise, if the value of the <em>@id</em> property in
<var>skolemizedNode</var> starts with "_:", preserve the existing blank node
identifier when skolemizing by setting the value of the <em>@id</em> property
in <var>skolemizedNode</var> to the concatenation of "urn:",
<var>urnScheme</var>, and the existing value of the <em>@id</em> property.
                </li>
                <li>
Append <var>skolemizedNode</var> to <var>skolemizedExpandedDocument</var>.
                </li>
              </ol>
            </li>
            <li>
Return <var>skolemizedExpandedDocument</var>.
            </li>
          </ol>
        </section>

        <section>
          <h4>skolemizeCompactJsonLd</h4>

          <p>
The following algorithm replaces all blank node identifiers in a compact
JSON-LD document with custom-scheme URNs. The required inputs are a compact
JSON-LD document (<var>document</var>) and a custom URN scheme
(<var>urnScheme</var>). The <var>document</var> is assumed to use only one
<em>@context</em> property at the top level of the document. Any additional
custom options (such as a document loader) can also be passed. It produces both
an expanded form of the skolemized JSON-LD document
(<var>skolemizedExpandedDocument</var> and a compact form of the skolemized
JSON-LD document (<var>skolemizedCompactDocument</var>) as output. The
skolemization used in this operation is intended to be reversible through the
use of the algorithm in Section <a href="#todeskolemizednquads"></a>.
          </p>

          <ol class="algorithm">
            <li>
Initialize <var>expanded</var> to the result of the JSON-LD
<a href="https://www.w3.org/TR/json-ld11-api/#expansion-algorithm">
Expansion Algorithm</a>, passing <var>document</var> and any custom
options.
            </li>
            <li>
Initialize <var>skolemizedExpandedDocument</var> to the result of the
algorithm in Section <a href="#skolemizeexpandedjsonld"></a>.
            </li>
            <li>
Initialize <var>skolemizedCompactDocument</var> to the result of the JSON-LD
<a href="https://www.w3.org/TR/json-ld11-api/#compaction-algorithm">
Compaction Algorithm</a>, passing <var>skolemizedExpandedDocument</var> and any
custom options.
            </li>
            <li>
Return an object with both <var>skolemizedExpandedDocument</var> and
<var>skolemizedCompactDocument</var>.
            </li>
          </ol>
        </section>

        <section>
          <h4>toDeskolemizedNQuads</h4>

          <p>
The following algorithm converts a skolemized JSON-LD document, such as one
created using the algorithm in Section <a href="#skolemizecompactjsonld"></a>,
to an array of deskolemized N-Quads. The required input is a JSON-LD document,
<em>skolemizedDocument</em>. Additional custom options (such as a document
loader) can be passed. An array of deskolemized N-Quad strings
(<var>deskolemizedNQuads</var>) is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize `skolemizedDataset` to the result of the
<a href="https://www.w3.org/TR/json-ld11-api/#deserialize-json-ld-to-rdf-algorithm">
Deserialize JSON-LD to RDF</a> algorithm, passing any custom options (such as a
document loader), to convert `skolemizedDocument` from JSON-LD to RDF in N-Quads
format.
            </li>
            <li>
Split `skolemizedDataset` into an array of individual N-Quads,
`skolemizedNQuads`.
            </li>
            <li>
Set <var>deskolemizedNQuads</var> to the result of the algorithm in Section
<a href="#deskolemizenquads"></a> with `skolemizedNQuads` and "custom-scheme:"
as parameters. Implementations MAY choose a different <em>urnScheme</em>
that is different than "custom-scheme:" so long as the same scheme name was
used to generate <em>skolemizedDocument</em>.
            </li>
            <li>
Return <var>deskolemizedNQuads</var>.
            </li>
          </ol>
        </section>

        <section>
          <h4>jsonPointerToPaths</h4>

          <p>
The following algorithm converts a JSON Pointer [[RFC6901]] to an array of paths
into a JSON tree. The required input is a JSON Pointer string
(<var>pointer</var>). An array of paths (<em>paths</em>) is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize `paths` to an empty array.
            </li>
            <li>
Initialize `splitPath` to an array by splitting <var>pointer</var> on the
"/" character and skipping the first, empty, split element. In Javascript
notation, this step is equivalent to the following code:
`pointer.split('/').slice(1)`
            </li>
            <li>
For each `path` in `splitPath`:
              <ol class="algorithm">
                <li>
If `path` does not include `~`, then add `path` to `paths`, converting it to
an integer if it parses as one, leaving it as a string if it does not.
                </li>
                <li>
Otherwise, unescape any JSON pointer escape sequences in `path` and add the
result to `paths`.
                </li>
              </ol>
            </li>
            <li>
Return `paths`.
            </li>
          </ol>
        </section>

        <section>
          <h4>createInitialSelection</h4>

          <p>
The following algorithm creates an initial selection (a fragment of a JSON-LD
document) based on a JSON-LD object. This is a helper function used within the
algorithm in Section <a href="#selectjsonld"></a>. The required input is a
JSON-LD object (<var>source</var>). A JSON-LD document fragment object
(<var>selection</var>) is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize <var>selection</var> to an empty object.
            </li>
            <li>
If <var>value</var> has an `id` that is not a blank node identifier, set
`selection.id` to its value. Note: All non-blank node identifiers in the path of
any JSON Pointer MUST be included in the selection, this includes any root
document identifier.
            </li>
            <li>
If <var>value</var>.`type` is set, set <var>selection</var>.`type` to its value.
Note: The selection MUST include all `type`s in the path of any JSON Pointer,
including any root document `type`.
            </li>
            <li>
Return <var>selection</var>.
            </li>
          </ol>
        </section>

        <section>
          <h4>selectPaths</h4>

          <p>
The following algorithm selects a portion of a compact JSON-LD document using
paths parsed from a parsed JSON Pointer. This is a helper function used within
the algorithm in Section <a href="#selectjsonld"></a>. The required inputs are an
array of paths (<var>paths</var>) parsed from a JSON Pointer, a compact JSON-LD
document (<var>document</var>), a selection document
(<var>selectionDocument</var>) to be populated, and an array of arrays
(<var>arrays</var>) for tracking selected arrays. This algorithm produces
no output; instead it populates the given <var>selectionDocument</var> with
any values selected via <var>paths</var>.
          </p>

          <ol class="algorithm">
            <li>
Initialize `parentValue` to `document`.
            </li>
            <li>
Initialize `value` to `parentValue`.
            </li>
            <li>
Initialize `selectedParent` to `selectionDocument`.
            </li>
            <li>
Initialize `selectedValue` to `selectedParent`.
            </li>
            <li>
For each `path` in `paths`:
              <ol class="algorithm">
                <li>
Set `selectedParent` to `selectedValue`.
                </li>
                <li>
Set `parentValue` to `value`.
                </li>
                <li>
Set `value` to `parentValue[path]`. If `value` is now undefined, throw an error
indicating that the JSON pointer does not match the given `document`.
                </li>
                <li>
Set `selectedValue` to `selectedParent[path]`.
                </li>
                <li>
If `selectedValue` is now undefined:
                  <ol class="algorithm">
                    <li>
If `value` is an array, set `selectedValue` to an empty array and append
`selectedValue` to <var>arrays</var>.
                    </li>
                    <li>
Otherwise, set `selectedValue` to an initial selection passing `value` as
`source` to the algorithm in Section <a href="#createinitialselection"></a>.
                    </li>
                    <li>
Set `selectedParent[path]` to `selectedValue`.
                    </li>
                  </ol>
                </li>
              </ol>
            </li>
            <li>
Note: With path traversal complete at the target value, the selected value will
now be computed.
            </li>
            <li>
If `value` is a literal, set `selectedValue` to `value`.
            </li>
            <li>
If `value` is an array, Set `selectedValue` to a copy of `value`.
            </li>
            <li>
In all other cases, set `selectedValue` to an object that merges a shallow copy of
`selectedValue` with a deep copy of `value`, e.g., `{...selectedValue,
&hellip;deepCopy(value)}`.
            </li>
            <li>
Get the last `path`, `lastPath`, from `paths`.
            </li>
            <li>
Set `selectedParent[lastPath]` to `selectedValue`.
            </li>
          </ol>
        </section>

        <section>
          <h4>selectJsonLd</h4>

          <p>
The following algorithm selects a portion of a compact JSON-LD document using
an array of JSON Pointers. The required inputs are an array of JSON Pointers
(<var>pointers</var>) and a compact JSON-LD document (<var>document</var>). The
<var>document</var> is assumed to use a JSON-LD context that aliases `@id`
and `@type` to `id` and `type`, respectively, and to use only one
<em>`@context`</em> property at the top level of the document. A new JSON-LD
document that represents a selection (<em>selectionDocument</em>) of the
original JSON-LD document is produced as output.
          </p>

          <ol class="algorithm">
            <li>
If `pointers` is empty, return `null`. This indicates nothing has been selected
from the original document.
            </li>
            <li>
Initialize `arrays` to an empty array. This variable will be used to track
selected sparse arrays to make them dense after all <var>pointers</var> have
been processed.
            </li>
            <li>
Initialize `selectionDocument` to an initial selection passing `document` as
`source` to the algorithm in Section <a href="#createinitialselection"></a>.
            </li>
            <li>
Set the value of the <em>`@context`</em> property in `selectionDocument` to
a copy of the value of the <em>`@context`</em> property in `document`.
            <li>
For each `pointer` in `pointers`, walk the document from root to the pointer
target value, building the `selectionDocument`:
              <ol class="algorithm">
                <li>
Parse the `pointer` into an array of paths, <var>paths</var>, using the
algorithm in Section <a href="#jsonpointertopaths"></a>.
                </li>
                <li>
Use the algorithm in Section <a href="#selectpaths"></a>, passing
<var>document</var>, <var>paths</var>, <var>selectionDocument</var>, and
<var>arrays</var>.
                </li>
              </ol>
            </li>
            <li>
For each <var>array</var> in <var>arrays</var>:
              <ol class="algorithm">
                <li>
Make <var>array</var> dense by removing any undefined elements between
elements that are defined.
                </li>
              </ol>
            </li>
            <li>
Return `selectionDocument`.
            </li>
          </ol>
        </section>

        <section>
          <h4>relabelBlankNodes</h4>

          <p>
The following algorithm relabels the blank node identifiers in an array of
N-Quad strings using a blank node label map. The required inputs are an array
of N-Quad strings (<var>nquads</var>) and a blank node label map
(<var>labelMap</var>). An array of N-Quad strings with relabeled blank node
identifiers (<var>relabeledNQuads</var>) is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Create a new array of N-Quad strings, <em>relabeledNQuads</em>.
            </li>
            <li>
For each N-Quad string, <em>s1</em>, in <var>nquads</var>:
              <ol class="algorithm">
                <li>
Create a new string, <em>s2</em>, such it that is a copy of <em>s1</em> except
each blank node identifier therein has been replaced with the value associated
with it as a key in <var>labelMap</var>.
                </li>
                <li>
Append <em>s2</em> to <em>relabeledNQuads</em>.
                </li>
              </ol>
            </li>
            <li>
Return <em>relabeledNQuads</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>selectCanonicalNQuads</h4>

          <p>
The following algorithm selects a portion of a skolemized compact JSON-LD
document using an array of JSON Pointers, and outputs the resulting canonical
N-Quads with any blank node labels replaced using the given label map. The
required inputs are an array of JSON Pointers (<var>pointers</var>), a
skolemized compact JSON-LD document (<var>skolemizedCompactDocument</var>),
and a blank node label map (<var>labelMap</var>). Additional custom options
(such as a document loader) can be passed. The <var>document</var> is assumed
to use a JSON-LD context that aliases `@id` and `@type` to `id` and `type`,
respectively, and to use only one <em>`@context`</em> property at the top level
of the document. An object containing the new JSON-LD document that represents
a selection of the original JSON-LD document (<var>selectionDocument</var>), an
array of deskolemized N-Quad strings (<var>deskolemizedNQuads</var>), and an
array of canonical N-Quads with replacement blank node labels (<var>nquads</var>)
is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize <var>selectionDocument</var> to the result of the algorithm
in Section <a href="#selectjsonld"></a>, passing
<var>skolemizedCompactDocument</var> as <em>document</em>, with
<var>pointers</var>.
            </li>
            <li>
Initialize <var>deskolemizedNQuads</var> to the result of the algorithm
in Section <a href="#todeskolemizednquads"></a>, passing
<var>selectionDocument</var> as <var>skolemizedCompactDocument</var>, with
any custom options.
            </li>
            <li>
Initialize <var>nquads</var> to the result of the algorithm
in Section <a href="#relabelblanknodes"></a> passing
<var>deskolemizedNQuads</var> as <var>nquads</var> and <var>labelMap</var>.
            </li>
            <li>
Return an object containing <var>selectionDocument</var>,
<var>deskolemizedNQuads</var>, and <var>nquads</var>.
            </li>
          </ol>
        </section>

        <section>
          <h4>canonicalizeAndGroup</h4>

          <p>
The following algorithm is used to output canonical N-Quad strings that match
custom selections of a compact JSON-LD document. It does this by canonicalizing
a compact JSON-LD document (replacing any blank node identifiers using a label
map) and grouping the resulting canonical N-Quad strings according to
the selection associated with each group. Each group will be defined using an
assigned name and array of JSON pointers. The JSON pointers will be used to
select portions of the skolemized document, such that the output can be
converted to canonical N-Quads to perform group matching.
          </p>

          <p>
The required inputs are a compact JSON-LD document (<var>document</var>),
a label map factory function (<var>labelMapFactoryFunction</var>), and a map
of named group definitions (<var>groupDefinitions</var>). Additional custom
options (such as a document loader) can be passed. The <var>document</var> is
assumed to use a JSON-LD context that aliases `@id` and `@type` to `id` and
`type`, respectively, and to use only one <em>`@context`</em> property at the top
level of the document. An object containing the created groups
(<var>groups</var>), the skolemized compact JSON-LD document
(<var>skolemizedCompactDocument</var>), the skolemized expanded JSON-LD
document (<var>skolemizedExpandedDocument</var>), the deskolemized N-Quad
strings (<var>deskolemizedNQuads</var>), the blank node label map
(<var>labelMap</var>), and the canonical N-Quad strings <var>nquads</var>, is
produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize <var>skolemizedExpandedDocument</var> and
<var>skolemizedCompactDocument</var> to their associated values in the
result of the algorithm in Section <a href="#skolemizecompactjsonld"></a>,
passing <var>document</var> and any custom options.
            </li>
            <li>
Initialize <var>deskolemizedNQuads</var> to the result of the
algorithm in Section <a href="#todeskolemizednquads"></a>, passing
<var>skolemizedExpandedDocument</var> and any custom options.
            </li>
            <li>
Initialize <var>nquads</var> and <var>labelMap</var> to their associated
values in the result of the algorithm in Section
<a href="#labelreplacementcanonicalizenquads"></a>, passing
<var>deskolemizedNQuads</var> as <var>nquads</var>,
<var>labelMapFactoryFunction</var>, and any custom options.
            </li>
            <li>
Initialize <var>selections</var> to a new map.
            </li>
            <li>
For each key (<var>name</var>) and value (<var>pointers</var>) entry in
<var>groupDefinitions</var>:
              <ol class="algorithm">
                <li>
Add an entry with a key of <var>name</var> and a value that is the result
of the algorithm in Section <a href="#selectcanonicalnquads"></a>, passing
<var>skolemizedCompactDocument</var> as <var>document</var>,
<var>pointers</var>, <var>labelMap</var>, and any custom options.
                </li>
              </ol>
            </li>
            <li>
Initialize <var>groups</var> to an empty object.
            </li>
            <li>
For each key (<var>name</var>) and value (<var>selectionResult</var>) entry in
<var>selections</var>:
              <ol class="algorithm">
                <li>
Initialize <var>matching</var> to an empty map.
                </li>
                <li>
Initialize <var>nonMatching</var> to an empty map.
                </li>
                <li>
Initialize <var>selectedNQuads</var> to <em>nquads</em> from
<var>selectionResult</var>.
                </li>
                <li>
Initialize <var>selectedDeskolemizedNQuads</var> from
<em>deskolemizedNQuads</em> from <var>selectionResult</var>.
                </li>
                <li>
For each element (<var>nq</var>) and index (<var>index</var>) in
<var>nquads</var>:
                  <ol class="algorithm">
                    <li>
Create a map entry, <var>entry</var>, with a key of <var>index</var> and a
value of <var>nq</var>.
                    </li>
                    <li>
If <var>selectedNQuads</var> includes <var>nq</var> then add <var>entry</var>
to <var>matching</var>; otherwise, add <var>entry</var> to <var>nonMatching</var>.
                    </li>
                  </ol>
                </li>
                <li>
Set <var>name</var> in <var>groups</var> to an object containing
<var>matching</var>, <var>nonMatching</var>, and
<var>selectedDeskolemizedNQuads</var> as <var>deskolemizedNQuads</var>.
                </li>
              </ol>
            </li>
            <li>
Return an object containing <var>groups</var>,
<var>skolemizedExpandedDocument</var>, <var>skolemizedCompactDocument</var>,
<var>deskolemizedNQuads</var>, <var>labelMap</var>, and <var>nquads</var>.
            </li>
          </ol>
        </section>

        <section>
          <h4>hashMandatoryNQuads</h4>

          <p>
The following algorithm cryptographically hashes an array of mandatory to
disclose N-Quads using a provided hashing API. The required input is an array of
mandatory to disclose N-Quads (<var>mandatory</var>) and a hashing function
(<var>hasher</var>). A cryptographic hash (<em>mandatoryHash</em>) is produced
as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize `bytes` to the UTF-8 representation of the joined `mandatory`
N-Quads.
            </li>
            <li>
Initialize `mandatoryHash` to the result of using `hasher` to hash `bytes`.
            </li>
            <li>
Return `mandatoryHash`.
            </li>
          </ol>
        </section>
      </section>


      <section>
        <h3>ecdsa-sd-2023 Functions</h3>

        <p class="issue" title="(AT RISK) Pending implementation feedback and security reviews.">
The Working Group is seeking implementer feedback on these cryptographic suite
functions as well as horizonal security review on the feature from parties at
W3C and IETF. Those reviews might result in significant changes to these
algorithms, or the removal of the algorithms from the specification during the
Candidate Recommendation phase.
        </p>

        <p>
This section contains subalgorithms that are useful to the `ecdsa-sd-2023`
cryptographic suite.
        </p>

        <section>
          <h4>serializeSignData</h4>

          <p>
The following algorithm serializes the data that is to be signed by the private
key associated with the base proof verification method. The required inputs are
the proof options hash (<var>proofHash</var>), the proof-scoped multikey-encoded
public key (<var>publicKey</var>), and the mandatory hash
(<var>mandatoryHash</var>). A single <em>sign data</em> value,
represented as series of bytes, is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Return the concatenation of <var>proofHash</var>, <var>publicKey</var>, and
<var>mandatoryHash</var>, in that order, as <em>sign data</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>serializeBaseProofValue</h4>

          <p>
The following algorithm serializes the base proof value, including the
base signature, public key, HMAC key, signatures, and mandatory pointers.
The required inputs are a base signature <var>baseSignature</var>, a public key
<var>publicKey</var>, an HMAC key <var>hmacKey</var>, an array of
<var>signatures</var>, and an array of <var>mandatoryPointers</var>.
A single <em>base proof</em> string value is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize a byte array, `proofValue`, that starts with the ECDSA-SD base proof
header bytes 0xd9, 0x5d, and 0x00.
            </li>
            <li>
Initialize `components` to an array with five elements containing the values of:
`baseSignature`, `publicKey`, `hmacKey`, `signatures`, and `mandatoryPointers`.
            </li>
            <li>
CBOR-encode `components` and append it to `proofValue`.
            </li>
            <li>
Initialize `baseProof` to a string with the multibase-base64url-no-pad-encoding
of `proofValue`. That is, return a string starting with "u" and ending with the
base64url-no-pad-encoded value of `proofValue`.
            </li>
            <li>
Return `baseProof` as <em>base proof</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>parseBaseProofValue</h4>

          <p>
The following algorithm parses the components of an `ecdsa-sd-2023` selective
disclosure base proof value. The required inputs are a proof value
(<var>proofValue</var>). A single object <em>parsed base proof</em>, containing
five elements, using the names "baseSignature", "publicKey", "hmacKey",
"signatures", and "mandatoryPointers", is produced  as output.
          </p>

          <ol class="algorithm">
            <li>
Ensure the `proofValue` string starts with `u`, indicating that it is a
multibase-base64url-no-pad-encoded value, throwing an error if it does not.
            </li>
            <li>
Initialize `decodedProofValue` to the result of base64url-no-pad-decoding the
substring after the leading `u` in `proofValue`.
            </li>
            <li>
Ensure that the `decodedProofValue` starts with the ECDSA-SD base proof header
bytes 0xd9, 0x5d, and 0x00, throwing an error if it does not.
            </li>
            <li>
Initialize `components` to an array that is the result of CBOR-decoding the
bytes that follow the three-byte ECDSA-SD base proof header. Ensure the result
is an array of five elements.
            </li>
            <li>
Return an object with properties set to the five elements, using the names
"baseSignature", "publicKey", "hmacKey", "signatures", and "mandatoryPointers",
respectively.
            </li>
          </ol>

        </section>

        <section>
          <h4>createDisclosureData</h4>

          <p>
The following algorithm creates data to be used to generate a derived proof. The
inputs include a JSON-LD document (<var>document</var>), an ECDSA-SD base proof
(<var>proof</var>), an array of JSON pointers to use to selectively disclose
statements (<var>selectivePointers</var>), and any custom JSON-LD API options,
such as a document loader). A single object, <em>disclosure data</em>, is
produced as output, which contains the "baseSignature", "publicKey",
"signatures" for "filteredSignatures", "labelMap", "mandatoryIndexes", and
"revealDocument" fields.
          </p>

          <ol class="algorithm">
            <li>
Initialize `baseSignature`, `publicKey`, `hmacKey`, `signatures`, and
`mandatoryPointers` to the values of the associated properties in the object
returned when calling the algorithm in Section
<a href="#parsebaseproofvalue"></a>, passing the `proofValue` from `proof`.
            </li>
            <li>
Initialize `hmac` to an HMAC API using `hmacKey`. The HMAC uses the same hash
algorithm used in the signature algorithm, i.e., SHA-256 for a P-256 curve.
            </li>
            <li>
Initialize `labelMapFactoryFunction` to the result of calling the
`createHmacIdLabelMapFunction` algorithm passing `hmac` as `HMAC`.
            </li>
            <li>
Initialize `combinedPointers` to the concatenation of `mandatoryPointers`
and `selectivePointers`.
            </li>
            <li>
Initialize `groupDefinitions` to a map with the following entries: key of
the string `"mandatory"` and value of `mandatoryPointers`, key of the string
`"selective"` and value of `selectivePointers`, and key of the string `"combined"`
and value of `combinedPointers`.
            </li>
            <li>
Initialize `groups` and `labelMap` to their associated values in the result
of calling the algorithm in Section
<a href="#canonicalizeandgroup"></a>, passing `document`,
`labelMapFactoryFunction`, `groupDefinitions`, and any custom
JSON-LD API options as parameters. Note: This step transforms the document into
an array of canonical N-Quad strings with pseudorandom blank node identifiers
based on `hmac`, and groups the N-Quad strings according to selections based on
JSON pointers.
            </li>
            <li>
Initialize `relativeIndex` to zero.
            </li>
            <li>
Initialize `mandatoryIndexes` to an empty array.
            </li>
            <li>
For each `absoluteIndex` in the keys in `groups.combined.matching`, convert the
absolute index of any mandatory N-Quad to an index relative to the combined
output that is to be revealed:
              <ol class="algorithm">
                <li>
If `groups.mandatory.matching` has `absoluteIndex` as a key, then append
`relativeIndex` to `mandatoryIndexes`.
                </li>
                <li>
Increment `relativeIndex`.
                </li>
              </ol>
            </li>
            <li>
Determine which signatures match a selectively disclosed statement, which
requires incrementing an index counter while iterating over all `signatures`,
skipping over any indexes that match the mandatory group.
              <ol class="algorithm">
                <li>
Initialize `index` to `0`.
                </li>
                <li>
Initialize `filteredSignatures` to an empty array.
                </li>
                <li>
For each `signature` in `signatures`:
                  <ol class="algorithm">
                    <li>
While `index` is in `groups.mandatory.matching`, increment `index`.
                    </li>
                    <li>
If `index` is in `groups.selective.matching`, add `signature` to
`filteredSignatures`.
                    </li>
                    <li>
Increment `index`.
                    </li>
                  </ol>
                </li>
              </ol>
            </li>
            <li>
Initialize <var>revealDocument</var> to the result of the "selectJsonLd"
algorithm, passing `document`, with `combinedPointers` as `pointers`.
            </li>
            <li>
Run the RDF Dataset Canonicalization Algorithm [[RDF-CANON]] on
the joined <var>combinedGroup.deskolemizedNQuads</var>, passing any custom
options, and get the canonical bnode identifier map, <var>canonicalIdMap</var>.
Note: This map includes the canonical blank node identifiers that a verifier
will produce when they canonicalize the reveal document.
            </li>
            <li>
Initialize <var>verifierLabelMap</var> to an empty map. This map will map
the canonical blank node identifiers the verifier will produce when they
canonicalize the revealed document to the blank node identifiers that were
originally signed in the base proof.
            </li>
            <li>
For each key (`inputLabel`) and value (`verifierLabel`) in `canonicalIdMap:
              <ol class="algorithm">
                <li>
Add an entry to `verifierLabelMap` using `verifierLabel` as the key and the
value associated with `inputLabel` as a key in `labelMap` as the value.
                </li>
              </ol>
            </li>
            <li>
Return an object with properties matching `baseSignature`, `publicKey`,
"signatures" for `filteredSignatures`, "verifierLabelMap" for `labelMap`,
`mandatoryIndexes`, and `revealDocument`.
            </li>
          </ol>

        </section>

        <section>
          <h4>compressLabelMap</h4>

          <p>
The following algorithm compresses a label map. The required inputs are
label map (<var>labelMap</var>). The output is a <em>compressed label map</em>.
          </p>

          <ol class="algorithm">
            <li>
Initialize `map` to an empty map.
            </li>
            <li>
For each entry (`k`, `v`) in `labelMap`:
              <ol class="algorithm">
                <li>
Add an entry to `map` with a key that is a base-10 integer parsed from the
characters following the "c14n" prefix in `k` and a value that is a byte array
resulting from base64url-no-pad-decoding the characters after the "u" prefix in
                `v`.
                </li>
              </ol>
            </li>
            <li>
Return `map` as <em>compressed label map</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>decompressLabelMap</h4>

          <p>
The following algorithm decompresses a label map. The required input is a
compressed label map (<var>compressedLabelMap</var>). The output is a
<em>decompressed label map</em>.
          </p>

          <ol class="algorithm">
            <li>
Initialize `map` to an empty map.
            </li>
            <li>
For each entry (`k`, `v`) in `compressedLabelMap`:
              <ol class="algorithm">
                <li>
Add an entry to `map` with a key that adds the prefix "c14n" to `k` and a value
that adds a prefix of "u" to the base64url-no-pad-encoded value for `v`.
                </li>
              </ol>
            </li>
            <li>
Return `map` as <em>decompressed label map</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>serializeDerivedProofValue</h4>

          <p>
The following algorithm serializes a derived proof value. The required inputs
are a base signature (<var>baseSignature</var>), public key
(<var>publicKey</var>), an array of signatures (<var>signatures</var>), a label
map (<var>labelMap</var>), and an array of mandatory indexes
(<var>mandatoryIndexes</var>). A single <em>derived proof</em> value, serialized
as a byte string, is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize `compressedLabelMap` to the result of calling the algorithm in
Section <a href="#compresslabelmap"></a>, passing `labelMap` as the parameter.
            </li>
            <li>
Initialize a byte array, `proofValue`, that starts with the ECDSA-SD disclosure
proof header bytes `0xd9`, `0x5d`, and `0x01`.
            </li>
            <li>
Initialize `components` to an array with five elements containing the values of:
`baseSignature`, `publicKey`, `signatures`, `compressedLabelMap`, and
`mandatoryIndexes`.
            </li>
            <li>
CBOR-encode `components` and append it to `proofValue`.
            </li>
            <li>
Return the <em>derived proof</em> as a string with the
multibase-base64url-no-pad-encoding of `proofValue`. That is, return a string
starting with "u" and ending with the base64url-no-pad-encoded value of
`proofValue`.
            </li>
          </ol>

        </section>

        <section>
          <h4>parseDerivedProofValue</h4>

          <p>
The following algorithm parses the components of the derived proof value.
The required inputs are a derived proof value (<var>proofValue</var>). A
A single <em>derived proof value</em> value object is produced as output, which
contains a set to five elements, using the names "baseSignature", "publicKey",
"signatures", "labelMap", and "mandatoryIndexes".
          </p>

          <ol class="algorithm">
            <li>
Ensure the `proofValue` string starts with `u`, indicating that it is a
multibase-base64url-no-pad-encoded value, throwing an error if it does not.
            </li>
            <li>
Initialize `decodedProofValue` to the result of base64url-no-pad-decoding the
substring after the leading `u` in `proofValue`.
            </li>
            <li>
Ensure that the `decodedProofValue` starts with the ECDSA-SD disclosure proof
header bytes `0xd9`, `0x5d`, and `0x01`, throwing an error if it does not.
            </li>
            <li>
Initialize `components` to an array that is the result of CBOR-decoding the
bytes that follow the three-byte ECDSA-SD disclosure proof header. Ensure the
result is an array of five elements. Ensure the result is an array of five
elements: a byte array of length 64, a byte array of length 36, an array of byte
arrays, each of length 64, a map of integers to byte arrays of length 32, and an
array of integers, throwing an error if not.
            </li>
            <li>
Replace the fourth element in `components` using the result of calling the
algorithm in Section <a href="#decompresslabelmap"></a>, passing the existing
fourth element of `components` as `compressedLabelMap`.
            </li>
            <li>
Return <em>derived proof value</em> as an object with properties set to the five
elements, using the names "baseSignature", "publicKey", "signatures",
"labelMap", and "mandatoryIndexes", respectively.
            </li>
          </ol>

        </section>

        <section>
          <h4>createVerifyData</h4>

          <p>
The following algorithm creates the data needed to perform verification of an
ECDSA-SD-protected <a>verifiable credential</a>. The inputs include a JSON-LD
document (<var>document</var>), an ECDSA-SD disclosure proof (<var>proof</var>),
and any custom JSON-LD API options, such as a document loader. A single
<em>verify data</em> object value is produced as output containing the following
fields: "baseSignature", "proofHash", "publicKey", "signatures", "nonMandatory",
and "mandatoryHash".
          </p>

          <ol class="algorithm">
            <li>
Initialize `proofHash` to the result of perform RDF Dataset Canonicalization
[[RDF-CANON]] on the proof options. The hash used is the same as the one used in
the signature algorithm, i.e., SHA-256 for a P-256 curve. Note: This step can be
performed in parallel; it only needs to be completed before this algorithm needs
to use the `proofHash` value.
            </li>
            <li>
Initialize `baseSignature`, `publicKey`, `signatures`, `labelMap`, and
`mandatoryIndexes`, to the values associated with their property names in the
object returned when calling the algorithm in Section
<a href="#parsederivedproofvalue"></a>, passing `proofValue` from `proof`.
            </li>
            <li>
Initialize `labelMapFactoryFunction` to the result of calling the
"createLabelMapFunction" algorithm.
            </li>
            <li>
Initialize `nquads` to the result of calling the "labelReplacementCanonicalize"
algorithm, passing `document`, `labelMapFactoryFunction`, and any custom
JSON-LD API options. Note: This step transforms the document into an array of
canonical N-Quads with pseudorandom blank node identifiers based on `labelMap`.
            </li>
            <li>
Initialize `mandatory` to an empty array.
            </li>
            <li>
Initialize `nonMandatory` to an empty array.
            </li>
            <li>
For each entry (`index`, `nq`) in `nquads`, separate the N-Quads into mandatory
and non-mandatory categories:
              <ol class="algorithm">
                <li>
If `mandatoryIndexes` includes `index`, add `nq` to `mandatory`.
                </li>
                <li>
Otherwise, add `nq` to `nonMandatory`.
                </li>
              </ol>
            </li>
            <li>
Initialize `mandatoryHash` to the result of calling the "hashMandatory"
primitive, passing `mandatory`.
            </li>
            <li>
Return an object with properties matching `baseSignature`, `proofHash`,
`publicKey`, `signatures`, `nonMandatory`, and `mandatoryHash`.
            </li>
          </ol>

        </section>

      </section>

      <section>
        <h3>ecdsa-sd-2023</h3>

        <p class="issue" title="(AT RISK) Pending implementation feedback and security reviews.">
The Working Group is seeking implementer feedback on this cryptographic suite
as well as horizonal security review on the feature from parties at W3C and
IETF. Those reviews might result in significant changes to this algorithm, or
the removal of the algorithm from the specification during the Candidate
Recommendation phase.
        </p>

        <p>
The `ecdsa-sd-2023` cryptographic suite takes an input document, canonicalizes
the document using the Universal RDF Dataset Canonicalization Algorithm
[[RDF-CANON]], and then cryptographically hashes and signs the output
resulting in the production of a data integrity proof. The algorithms in this
section also include the verification of such a data integrity proof.
        </p>

        <section>
          <h4>Add Base Proof (ecdsa-sd-2023)</h4>

          <p>
To generate a base proof, the algorithm in
<a href="https://www.w3.org/TR/vc-data-integrity/#add-proof">
Section 4.1: Add Proof</a> in the Data Integrity
[[VC-DATA-INTEGRITY]] specification MUST be executed.
For that algorithm, the cryptographic suite specific
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-transformation-algorithm">
transformation algorithm</a> is defined in Section
<a href="#base-proof-transformation-ecdsa-sd-2023"></a>, the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-hashing-algorithm">
hashing algorithm</a> is defined in Section <a href="#base-proof-hashing-ecdsa-sd-2023"></a>,
and the
<a href="https://www.w3.org/TR/vc-data-integrity/#dfn-proof-serialization-algorithm">
proof serialization algorithm</a> is defined in Section
<a href="#base-proof-serialization-ecdsa-sd-2023"></a>.
          </p>
        </section>

        <section>
          <h4>Base Proof Transformation (ecdsa-sd-2023)</h4>

          <p>
The following algorithm specifies how to transform an unsecured input document
into a transformed document that is ready to be provided as input to the
hashing algorithm in Section <a href="#base-proof-hashing-ecdsa-sd-2023"></a>.
          </p>

          <p>
Required inputs to this algorithm are an
<a data-cite="vc-data-integrity#dfn-unsecured-data-document">
unsecured data document</a> (<var>unsecuredDocument</var>) and
transformation options (<var>options</var>). The
transformation options MUST contain a type identifier for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>), a cryptosuite
identifier (<var>cryptosuite</var>), and a verification method
(<var>verificationMethod</var>). The transformation options MUST contain an
array of mandatory JSON pointers (<var>mandatoryPointers</var>) and MAY contain
additional options, such as a JSON-LD document loader. A <em>transformed data
document</em> is produced as output. Whenever this algorithm encodes strings, it
MUST use UTF-8 encoding.
          </p>

          <ol class="algorithm">
            <li>
Initialize `hmac` to an HMAC API using a locally generated and exportable HMAC
key. The HMAC uses the same hash algorithm used in the signature algorithm,
which is detected via the <var>verificationMethod</var> provided to the
function. i.e., SHA-256 for a P-256 curve.
            </li>
            <li>
Initialize `labelMapFactoryFunction` to the result of calling the
`createHmacIdLabelMapFunction` algorithm passing `hmac` as `HMAC`.
            </li>
            <li>
Initialize `groupDefinitions` to a map with an entry with a key of the string
"mandatory" and a value of <var>mandatoryPointers</var>.
            </li>
            <li>
Initialize `groups` to the result of calling the algorithm in Section
<a href="#canonicalizeandgroup"></a>, passing `unsecuredDocument` as `document`,
`labelMapFactoryFunction`, `groupDefinitions`, and any custom JSON-LD API
options. Note: This step transforms the document into an array of canonical
N-Quads with pseudorandom blank node identifiers based on `hmac`, and groups
the N-Quad strings according to selections based on JSON pointers.
            </li>
            <li>
Initialize `mandatory` to the values in the `groups.mandatory.matching` map.
            </li>
            <li>
Initialize `nonMandatory` to the values in the `groups.mandatory.nonMatching`
map.
            </li>
            <li>
Initialize `hmacKey` to the result of exporting the HMAC key from `hmac`.
            </li>
            <li>
Return an object with "mandatoryPointers" set to `mandatoryPointers`,
"mandatory" set to `mandatory`, "nonMandatory" set to `nonMandatory`,
and "hmacKey" set to `hmacKey`.
            </li>
          </ol>
        </section>

        <section>
          <h4>Base Proof Hashing (ecdsa-sd-2023)</h4>

          <p>
The following algorithm specifies how to cryptographically hash a
<em>transformed data document</em> and <em>proof configuration</em>
into cryptographic hash data that is ready to be provided as input to the
algorithms in Section <a href="#base-proof-serialization-ecdsa-sd-2023"></a>.
          </p>

          <p>
The required inputs to this algorithm are a <em>transformed data document</em>
(<var>transformedDocument</var>) and <em>canonical proof configuration</em>
(<var>canonicalProofConfig</var>). A <em>hash data</em> value represented
as an object is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize `proofHash` to the result of calling the RDF Dataset Canonicalization
algorithm [[RDF-CANON]] on `canonicalProofConfig` and then cryptographically
hashing the result using the same hash that is used by the signature algorithm,
i.e., SHA-256 for a P-256 curve. Note: This step can be performed in parallel;
it only needs to be completed before this algorithm terminates as the result is
part of the return value.
            </li>
            <li>
Initialize `mandatoryHash` to the result of calling the the algorithm in Section
<a href="#hashmandatorynquads"></a>, passing
<var>transformedDocument</var>.`mandatory`.
            </li>
            <li>
Initialize `hashData` as a deep copy of <var>transformedDocument</var> and
add `proofHash` as "proofHash" and `mandatoryHash` as "mandatoryHash" to that
object.
            </li>
            <li>
Return `hashData` as <em>hash data</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Base Proof Configuration (ecdsa-sd-2023)</h4>

          <p>
The following algorithm specifies how to generate a
<em>proof configuration</em> from a set of <em>proof options</em>
that is used as input to the
<a href="#base-proof-hashing-ecdsa-sd-2023">base proof hashing algorithm</a>.
          </p>

          <p>
The required inputs to this algorithm are <em>proof options</em>
(<var>options</var>). The <em>proof options</em> MUST contain a type identifier
for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>) and MUST contain a cryptosuite
identifier (<var>cryptosuite</var>). A <em>proof configuration</em>
object is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Let <var>proofConfig</var> be an empty object.
            </li>
            <li>
Set <var>proofConfig</var>.<var>type</var> to
<var>options</var>.<var>type</var>.
            </li>
            <li>
If <var>options</var>.<var>cryptosuite</var> is set, set
<var>proofConfig</var>.<var>cryptosuite</var> to its value.
            </li>
            <li>
If <var>options</var>.<var>type</var> is not set to `DataIntegrityProof` and
<var>proofConfig</var>.<var>cryptosuite</var> is not set to `ecdsa-sd-2023`, an
`INVALID_PROOF_CONFIGURATION` error MUST be raised.
            </li>
            <li>
Set <var>proofConfig</var>.<var>created</var> to
<var>options</var>.<var>created</var>. If the value is not a valid
[[XMLSCHEMA11-2]] datetime, an `INVALID_PROOF_DATETIME` error MUST be raised.
            </li>
            <li>
Set <var>proofConfig</var>.<var>verificationMethod</var> to
<var>options</var>.<var>verificationMethod</var>.
            </li>
            <li>
Set <var>proofConfig</var>.<var>proofPurpose</var> to
<var>options</var>.<var>proofPurpose</var>.
            </li>
            <li>
Set <var>proofConfig</var>.<var>@context</var> to
<var>unsecuredDocument</var>.<var>@context</var>.
            </li>
            <li>
Let <var>canonicalProofConfig</var> be the result of applying the
Universal RDF Dataset Canonicalization Algorithm
[[RDF-CANON]] to the <var>proofConfig</var>.
            </li>
            <li>
Return <var>canonicalProofConfig</var>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Base Proof Serialization (ecdsa-sd-2023)</h4>

          <p>
The following algorithm specifies how to create a base proof; called by an
issuer of an ECDSA-SD-protected Verifiable Credential. The base proof is to be
given only to the holder, who is responsible for generating a derived proof from
it, exposing only selectively disclosed details in the proof to a verifier. This
algorithm is designed to be used in conjunction with the algorithms defined
in the Data Integrity [[VC-DATA-INTEGRITY]] specification,
<a data-cite="vc-data-integrity#algorithms">
Section 4: Algorithms</a>. Required inputs are
cryptographic hash data (<var>hashData</var>) and
<em>proof options</em> (<var>options</var>). The
<em>proof options</em> MUST contain a type identifier for the
<a data-cite="vc-data-integrity#dfn-cryptosuite">
cryptographic suite</a> (<var>type</var>) and MAY contain a cryptosuite
identifier (<var>cryptosuite</var>). A single <em>digital proof</em> value
represented as series of bytes is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize `proofHash`, `mandatoryPointers`, `mandatoryHash`, `nonMandatory`,
and `hmacKey` to the values associated with their property names
<var>hashData</var>.
            </li>
            <li>
Initialize `proofScopedKeyPair` to a locally generated P-256 ECDSA key pair.
Note: This key pair is scoped to the specific proof; it is not used for anything
else and the private key will be destroyed when this algorithm terminates.
            </li>
            <li>
Initialize `signatures` to an array where each element holds the result of
digitally signing the UTF-8 representation of each N-Quad string in
`nonMandatory`, in order. The digital signature algorithm is ES256, i.e., uses a
P-256 curve over a SHA-256 digest, and uses the private key from
`proofScopedKeyPair`. Note: This step generates individual signatures for each
statement that can be selectively disclosed using a local, proof-scoped key pair
that binds them together; this key pair will be bound to the proof by a
signature over its public key using the private key associated with the base
proof verification method.
            </li>
            <li>
Initialize `publicKey` to the multikey expression of the public key exported
from `proofScopedKeyPair`. That is, an array of bytes starting with the bytes
0x80 and 0x24 (which is the multikey p256-pub header (0x1200) expressed as a
varint) followed by the compressed public key bytes (the compressed header with
`2` for an even `y` coordinate and `3` for an odd one followed by the `x`
coordinate of the public key).
            </li>
            <li>
Initialize `toSign` to the result of calling the algorithm in Section
<a href="#serializesigndata"></a>, passing `proofHash`, `publicKey`, and
`mandatoryHash` as parameters to the algorithm.
            </li>
            <li>
Initialize `baseSignature` to the result of digitally signing `toSign` using the
private key associated with the base proof verification method.
            </li>
            <li>
Initialize `proofValue to the result of calling the algorithm in Section
<a href="#serializebaseproofvalue"></a>, passing `baseSignature`,
`publicKey`, `hmacKey`, `signatures`, and `mandatoryPointers` as parameters
to the algorithm.
            </li>
            <li>
Return `proofValue` as <em>digital proof</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Add Derived Proof (ecdsa-sd-2023)</h4>

          <p>
The following algorithm creates a selective disclosure derived proof; called by
a holder of an `ecdsa-sd-2023`-protected <a>verifiable credential</a>.
The derived proof is to be given to the <a>verifier</a>. The inputs include a
JSON-LD document (<var>document</var>), an ECDSA-SD base proof
(<var>proof</var>), an array of JSON pointers to use to selectively disclose
statements (<var>selectivePointers</var>), and any custom JSON-LD API options,
such as a document loader. A single <em>selectively revealed document</em>
value, represented as an object, is produced as output.
          </p>

          <ol>
            <li>
Initialize `baseSignature`, `publicKey`, `signatures`, `labelMap`,
`mandatoryIndexes`, `revealDocument` to the values associated with their
property names in the object returned when calling the algorithm in
Section <a href="#createdisclosuredata"></a>, passing the `document`, `proof`,
`selectivePointers`, and any custom JSON-LD API options, such as a document
loader.
            </li>
            <li>
Initialize `newProof` to a shallow copy of `proof`.
            </li>
            <li>
Replace `proofValue` in `newProof` with the result of calling the algorithm
in Section <a href="#serializederivedproofvalue"></a>, passing
`baseSignature`, `publicKey`, `signatures`, `labelMap`, and `mandatoryIndexes`.
            </li>
            <li>
Set the value of the "proof" property in `revealDocument` to `newProof`.
            </li>
            <li>
Return `revealDocument` as the <em>selectively revealed document</em>.
            </li>
          </ol>

        </section>

        <section>
          <h4>Verify Derived Proof (ecdsa-sd-2023)</h4>

          <p>
The following algorithm attempts verification of an `ecdsa-sd-2023` derived
proof. This algorithm is called by a verifier of an ECDSA-SD-protected
<a>verifiable credential</a>. The inputs include a JSON-LD document
(<var>document</var>), an ECDSA-SD disclosure proof (<var>proof</var>), and any
custom JSON-LD API options, such as a document loader. A single boolean
<em>verification result</em> value is produced as output.
          </p>

          <ol class="algorithm">
            <li>
Initialize `baseSignature`, `proofHash`, `publicKey`, `signatures`,
`nonMandatory`, and `mandatoryHash` to the values associated with their property
names in the object returned when calling the algorithm in Section
<a href="#createverifydata"></a>, passing the `document`, `proof`, and any
custom JSON-LD API options, such as a document loader.
            </li>
            <li>
If the length of `signatures` does not match the length of `nonMandatory`, throw
an error indicating that the signature count does not match the non-mandatory
message count.
            </li>
            <li>
Initialize `publicKeyBytes` to the public key bytes expressed in `publicKey`.
Instructions on how to decode the public key value can be found in Section
<a href="#multikey"></a>.
            </li>
            <li>
Initialize `toVerify` to the result of calling the algorithm in Setion
<a href="#serializesigndata"></a>, passing `proofHash`, `publicKey`, and
`mandatoryHash`.
            </li>
            <li>
Initialize `verificationResult` be the result of applying the verification
algorithm of the Elliptic Curve Digital Signature Algorithm (ECDSA) [FIPS-186-5],
with `toVerify` as the data to be verified against the `baseSignature` using
the public key specified by `publicKeyBytes`. If `verificationResult` is
`false`, return `false`.
            </li>
            <li>
For every entry (`index`, `signature`) in `signatures`, verify every signature
for every selectively disclosed (non-mandatory) statement:
              <ol class="algorithm">
                <li>
Initialize `verificationResult` to the result of applying the verification
algorithm Elliptic Curve Digital Signature Algorithm (ECDSA) [FIPS-186-5], with
the UTF-8 representation of the value at `index` of `nonMandatory` as the data
to be verified against `signature` using the public key specified by
`publicKeyBytes`.
                </li>
                <li>
If `verificationResult` is `false`, return `false`.
                </li>
              </ol>
            </li>
            <li>
Return `verificationResult` as <em>verification result</em>.
            </li>
          </ol>

        </section>

      </section>

    </section>
    <section class="informative">
      <h2>Security Considerations</h2>
      <p>
The security (integrity/authenticity) of a verifiable credential
signed by a digital signature algorithm is dependent on a number of factors
including:
      </p>
      <ul>
        <li>
the correct application of the signature algorithm to a verifiable
credential (this specification)
        </li>
        <li>
the choice of signature algorithm (ECDSA) and its parameters (P-256, P-384)
        </li>
        <li>
the correct implementation and usage of the signature algorithm,
particularly with respect to well-known problem areas
        </li>
        <li>
the proper management of the private and public keys used for
signing and verification
        </li>
      </ul>
      <p>
In the following sections, we review these important points and direct
the reader to additional information.
      </p>
      <section class="informative">
        <h3>Choice of ECDSA and Parameters</h3>
        <p>
The ECDSA signature scheme has the <strong>EUF-CMA</strong>
(<em>existential unforgeability under chosen message attacks</em>) security
property. This property guarantees that any efficient adversary who has the
public key <em>pk</em> of the signer and received an arbitrary number of
signatures on messages of its choice (in an adaptive manner) cannot output a
valid signature for a new message (except with negligible probability).
        </p>
        <p>
<strong>SUF-CMA</strong> (<em>strong unforgeability under chosen
message attacks</em>) is a stronger notion than <em>EUF-CMA</em>. It guarantees
that for any efficient adversary who has the public key <em>pk</em> of the
signer and received an arbitrary number of signatures on messages of its
choice, it cannot output a new valid signature pair for a new message
nor a new signature for an old message (except with negligible probability).
 ECDSA signature scheme does <strong>not</strong> have the SUF-CMA property,
 while other schemes such as EdDSA [[FIPS-186-5]] do.
        </p>
        <p>
Per [[NIST-SP-800-57-Part-1]] in the absence of large scale quantum
computers a <em>security strength</em> level of 128 bits requires a key size
of approximately 256 bits while a security strength level of 192 bits requires
a key size of 384 bits. [[NIST-SP-800-186]] recommendations includes curves
P-256 and P-384 at these respective security strength levels.
        </p>
      </section>
      <section class="informative">
        <h3>Implementation Considerations for ECDSA Algorithms</h3>
        <p>
The ECDSA algorithm as detailed in [[FIPS-186-5]] states: &quot;A
new secret random number <em>k</em>, 0 < <em>k</em> < <em>n</em>,
<strong>shall</strong> be generated prior to the generation
of each digital signature for use during the signature generation process.&quot;
The failure to properly generate this <em>k</em> value has lead to some highly
publicized integrity breaches in widely deployed systems. To counter this problem,
a hash-based method of determining the secret number <em>k</em>, called
<em>Deterministic ECDSA</em>, is given in [[FIPS-186-5]] and [[RFC6979]].
Verification of a ECDSA signature is independent of the method of generating
<em>k</em>. Hence it is generally recommended to use <em>Deterministic
ECDSA</em> unless other requirements dictate otherwise.
        </p>
      </section>
      <section class="informative">
        <h3>Key Management</h3>
        <p>
The security of the ECDSA algorithm is dependent on the quality and
protection of its <em>private signing key</em>. Guidance in the management of
cryptographic keys is a large subject and the reader is referred to
[[NIST-SP-800-57-Part-1]] for more extensive recommendations and discussion.
As strongly recommended in both [[FIPS-186-5]] and [[NIST-SP-800-57-Part-1]], an ECDSA
private signing key is not to be used for any other purpose
than ECDSA signatures.
        </p>
        <p>
ECDSA private signing keys and <em>public verification keys</em> are strongly
advised to have limited <em>cryptoperiods</em> [[NIST-SP-800-57-Part-1]], where
a <em>cryptoperiod</em> is &quot;the time span during which a specific key is
authorized for use by legitimate entities or the keys for a given system will
remain in effect.&quot; [[NIST-SP-800-57-Part-1]] gives extensive
guidance on cryptoperiods for different key types under different situations
and generally recommends a 1-3 year cryptoperiod for a private signing key.
        </p>
        <p>
To deal with potential private key compromises, [[NIST-SP-800-57-Part-1]]
gives recommendations for protective measures, harm reduction, and revocation.
Although we have been emphasizing the security of the private signing key,
assurance of public key validity is highly recommended on all public keys
before using them, per [[NIST-SP-800-57-Part-1]].
        </p>
      </section>
      <section>
        <h3>Split Key Formats From Cryptosuites</h3>

        <p class="issue">
Ensuring that cryptographic suites are versioned and tightly scoped to a very
small set of possible key types and signature schemes (ideally one key type and
size and one signature output type) is a design goal for most Data Integrity
cryptographic suites. Historically, this has been done by defining both the
key type and the cryptographic suite that uses the key type in the same
specification. The downside of doing so, however, is that there might be a
proliferation of different key types in multikey that result in different
cryptosuites defining the same key material differently. For example, one
cryptosuite might use compressed Curve P-256 keys while another uses
uncompressed values. If that occurs, it will harm interoperability. It will be
important in the coming months to years to ensure that this does not happen
by fully defining the multikey format in a separate specification so
cryptosuite specifications, such as this one, can refer to the multikey
specification, thus reducing the chances of multikey type proliferation and
improving the chances of maximum interoperability for the multikey format.
        </p>

      </section>
    </section>

    <section>
      <h2>Privacy Considerations</h2>
      <p>
The following section describes privacy considerations that developers
implementing this specification should be aware of in order to avoid violating
privacy assumptions.
      </p>

      <p class="issue">
This cryptography suite does not provide for selective disclosure or
unlinkability. If signatures are re-used, they can be used as correlatable data.
      </p>
    </section>

    </section>
    <section class="appendix informative">
      <h2>Test Vectors</h2>
      <p class="note">
All test vectors are produced using <em>Deterministic ECDSA</em>. The
implementation was validated against the test vectors in [[RFC6979]].
      </p>
      <p class="issue" title="Cryptosuite naming">
The group is debating the names used for the cryptosuite identifiers in <a href="https://github.com/w3c/vc-data-integrity/issues/38">VC Data Integrity issue #38</a>. Cryptosuite identifiers might change in the future.
      </p>
      <section>
        <h3>Representation: ecdsa-rdfc-2019, with curve P-256</h3>
        <p>
The signer needs to generate a private/public key pair with the private key used
for signing and the public key made available for verification. The
[[MULTIBASE]]/[[MULTICODEC]] representation for the public key, <code>p256-pub</code>,
and the representation for the private key, <code>p256-priv</code>, are shown below.
        </p>
        <pre class="example nohighlight" title="Private and Public keys for Signature"
        data-include="TestVectors/p256KeyPair.json"
        data-include-format="text">
        </pre>

        <p>
Signing begins with a credential without an attached proof, which is converted
to canonical form, which is then hashed, as shown in the following three examples.
        </p>

        <pre class="example nohighlight" title="Credential without Proof" data-include="TestVectors/unsigned.json"
        data-include-format="text"></pre>

        <pre class="example nohighlight" title="Canonical Credential without Proof" data-include="TestVectors/ecdsa-2019-p256/canonDocECDSAP256.txt"
        data-include-format="text"></pre>


        <pre class="example nohighlight" title="Hash of Canonical Credential without Proof (hex)"
        data-include="TestVectors/ecdsa-2019-p256/docHashECDSAP256.txt" data-include-format="text"></pre>

        <p>
The next step is to take the proof options document, convert it to canonical form,
and obtain its hash, as shown in the next three examples.
        </p>

        <pre class="example nohighlight" title="Proof Options Document"
        data-include="TestVectors/ecdsa-2019-p256/proofConfigECDSAP256.json" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Canonical Proof Options Document"
        data-include="TestVectors/ecdsa-2019-p256/proofCanonECDSAP256.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Hash of Canonical Proof Options Document (hex)"
        data-include="TestVectors/ecdsa-2019-p256/proofHashECDSAP256.txt" data-include-format="text"></pre>

        <p>
Finally, we concatenate the hash of the proof options followed by the hash of the credential without proof, use the private key with the combined hash to
compute the ECDSA signature, and then base58-btc encode the signature.
        </p>

        <pre class="example nohighlight" title="Combine hashes of Proof Options and Credential (hex)"
        data-include="TestVectors/ecdsa-2019-p256/combinedHashECDSAP256.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Signature of Combined Hashes (hex)"
        data-include="TestVectors/ecdsa-2019-p256/sigHexECDSAP256.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Signature of Combined Hashes base58-btc"
        data-include="TestVectors/ecdsa-2019-p256/sigBTC58ECDSAP256.txt" data-include-format="text"></pre>

        <p>Assemble the signed credential with the following two steps:</p>
        <ol>
          <li>
Add the <code>proofValue</code> field with the previously computed base58-btc
value to the proof options document.
          </li>
          <li>
Set the <code>proof</code> field of the credential to the augmented proof
option document.
          </li>
        </ol>

        <pre class="example nohighlight" title="Signed Credential"
        data-include="TestVectors/ecdsa-2019-p256/signedECDSAP256.json" data-include-format="text"></pre>
      </section>
      <section>
        <h3>Representation: ecdsa-rdfc-2019, with curve P-384</h3>
        <p>
The signer needs to generate a private/public key pair with the private key used
for signing and the public key made available for verification. The
[[MULTIBASE]]/[[MULTICODEC]] representation for the public key, <code>p384-pub</code>,
and the representation for the private key, <code>p384-priv</code>, are shown below.
        </p>
        <pre class="example nohighlight" title="Private and Public keys for Signature"
        data-include="TestVectors/p384KeyPair.json"
        data-include-format="text">
        </pre>

        <p>
Signing begins with a credential without an attached proof, which is converted
to canonical form, and then hashed, as shown in the following three examples.
        </p>

        <pre class="example nohighlight" title="Credential without Proof" data-include="TestVectors/unsigned.json"
        data-include-format="text"></pre>

        <pre class="example nohighlight" title="Canonical Credential without Proof" data-include="TestVectors/ecdsa-2019-p384/canonDocECDSAP384.txt"
        data-include-format="text"></pre>


        <pre class="example nohighlight" title="Hash of Canonical Credential without Proof (hex)"
        data-include="TestVectors/ecdsa-2019-p384/docHashECDSAP384.txt" data-include-format="text"></pre>

        <p>
The next step is to take the proof options document, convert it to canonical form,
and obtain its hash, as shown in the next three examples.
        </p>

        <pre class="example nohighlight" title="Proof Options Document"
        data-include="TestVectors/ecdsa-2019-p384/proofConfigECDSAP384.json" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Canonical Proof Options Document"
        data-include="TestVectors/ecdsa-2019-p384/proofCanonECDSAP384.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Hash of Canonical Proof Options Document (hex)"
        data-include="TestVectors/ecdsa-2019-p384/proofHashECDSAP384.txt" data-include-format="text"></pre>

        <p>
Finally, we concatenate the hash of the proof options followed by the hash of the credential without proof, use the private key with the combined hash to
compute the ECDSA signature, and then base58-btc encode the signature.
        </p>

        <pre class="example nohighlight" title="Combine hashes of Proof Options and Credential (hex)"
        data-include="TestVectors/ecdsa-2019-p384/combinedHashECDSAP384.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Signature of Combined Hashes (hex)"
        data-include="TestVectors/ecdsa-2019-p384/sigHexECDSAP384.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Signature of Combined Hashes base58-btc"
        data-include="TestVectors/ecdsa-2019-p384/sigBTC58ECDSAP384.txt" data-include-format="text"></pre>

        <p>Assemble the signed credential with the following two steps:</p>
        <ol>
          <li>
Add the <code>proofValue</code> field with the previously computed base58-btc
value to the proof options document.
          </li>
          <li>
Set the <code>proof</code> field of the credential to the augmented proof
option document.
          </li>
        </ol>

        <pre class="example nohighlight" title="Signed Credential"
        data-include="TestVectors/ecdsa-2019-p384/signedECDSAP384.json" data-include-format="text"></pre>
      </section>
      <section>
        <h3>Representation: ecdsa-jcs-2019 with curve P-256</h3>
        <p>
The signer needs to generate a private/public key pair with the private key used
for signing and the public key made available for verification. The
[[MULTIBASE]]/[[MULTICODEC]] representation for the public key, <code>p256-pub</code>,
and the representation for the private key, <code>p256-priv</code>, are shown below.
        </p>
        <pre class="example nohighlight" title="Private and Public keys for Signature"
        data-include="TestVectors/p256KeyPair.json"
        data-include-format="text">
        </pre>

        <p>
Signing begins with a credential without an attached proof, which is converted
to canonical form, which is then hashed, as shown in the following three examples.
        </p>

        <pre class="example nohighlight" title="Credential without Proof" data-include="TestVectors/unsigned.json"
        data-include-format="text"></pre>

        <pre class="example nohighlight" title="Canonical Credential without Proof" data-include="TestVectors/jcs-ecdsa-2019-p256/canonDocJCSECDSAP256.txt"
        data-include-format="text"></pre>


        <pre class="example nohighlight" title="Hash of Canonical Credential without Proof (hex)"
        data-include="TestVectors/jcs-ecdsa-2019-p256/docHashJCSECDSAP256.txt" data-include-format="text"></pre>

        <p>
The next step is to take the proof options document, convert it to canonical form,
and obtain its hash, as shown in the next three examples.
        </p>

        <pre class="example nohighlight" title="Proof Options Document"
        data-include="TestVectors/jcs-ecdsa-2019-p256/proofConfigJCSECDSAP256.json" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Canonical Proof Options Document"
        data-include="TestVectors/jcs-ecdsa-2019-p256/proofCanonJCSECDSAP256.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Hash of Canonical Proof Options Document (hex)"
        data-include="TestVectors/jcs-ecdsa-2019-p256/proofHashJCSECDSAP256.txt" data-include-format="text"></pre>

        <p>
Finally, we concatenate the hash of the proof options followed by the hash of the credential without proof, use the private key with the combined hash to
compute the ECDSA signature, and then base58-btc encode the signature.
        </p>

        <pre class="example nohighlight" title="Combine hashes of Proof Options and Credential (hex)"
        data-include="TestVectors/jcs-ecdsa-2019-p256/combinedHashJCSECDSAP256.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Signature of Combined Hashes (hex)"
        data-include="TestVectors/jcs-ecdsa-2019-p256/sigHexJCSECDSAP256.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Signature of Combined Hashes base58-btc"
        data-include="TestVectors/jcs-ecdsa-2019-p256/sigBTC58JCSECDSAP256.txt" data-include-format="text"></pre>

        <p>Assemble the signed credential with the following two steps:</p>
        <ol>
          <li>
Add the <code>proofValue</code> field with the previously computed base58-btc
value to the proof options document.
          </li>
          <li>
Set the <code>proof</code> field of the credential to the augmented proof
option document.
          </li>
        </ol>

        <pre class="example nohighlight" title="Signed Credential"
        data-include="TestVectors/jcs-ecdsa-2019-p256/signedJCSECDSAP256.json" data-include-format="text"></pre>
      </section>
      <section>
        <h3>Representation: ecdsa-jcs-2019 with curve P-384</h3>
        <p>
The signer needs to generate a private/public key pair with the private key used
for signing and the public key made available for verification. The
[[MULTIBASE]]/[[MULTICODEC]] representation for the public key, <code>p384-pub</code>,
and the representation for the private key, <code>p384-priv</code>, are shown below.
        </p>
        <pre class="example nohighlight" title="Private and Public keys for Signature"
        data-include="TestVectors/p384KeyPair.json"
        data-include-format="text">
        </pre>

        <p>
Signing begins with a credential without an attached proof, which is converted
to canonical form, which is then hashed, as shown in the following three examples.
        </p>

        <pre class="example nohighlight" title="Credential without Proof" data-include="TestVectors/unsigned.json"
        data-include-format="text"></pre>

        <pre class="example nohighlight" title="Canonical Credential without Proof" data-include="TestVectors/jcs-ecdsa-2019-p384/canonDocJCSECDSAP384.txt"
        data-include-format="text"></pre>


        <pre class="example nohighlight" title="Hash of Canonical Credential without Proof (hex)"
        data-include="TestVectors/jcs-ecdsa-2019-p384/docHashJCSECDSAP384.txt" data-include-format="text"></pre>

        <p>
The next step is to take the proof options document, convert it to canonical form,
and obtain its hash, as shown in the next three examples.
        </p>

        <pre class="example nohighlight" title="Proof Options Document"
        data-include="TestVectors/jcs-ecdsa-2019-p384/proofConfigJCSECDSAP384.json" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Canonical Proof Options Document"
        data-include="TestVectors/jcs-ecdsa-2019-p384/proofCanonJCSECDSAP384.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Hash of Canonical Proof Options Document (hex)"
        data-include="TestVectors/jcs-ecdsa-2019-p384/proofHashJCSECDSAP384.txt" data-include-format="text"></pre>

        <p>
Finally, we concatenate the hash of the proof options followed by the hash of the credential without proof, use the private key with the combined hash to
compute the ECDSA signature, and then base58-btc encode the signature.
        </p>

        <pre class="example nohighlight" title="Combine hashes of Proof Options and Credential (hex)"
        data-include="TestVectors/jcs-ecdsa-2019-p384/combinedHashJCSECDSAP384.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Signature of Combined Hashes (hex)"
        data-include="TestVectors/jcs-ecdsa-2019-p384/sigHexJCSECDSAP384.txt" data-include-format="text"></pre>

        <pre class="example nohighlight" title="Signature of Combined Hashes base58-btc"
        data-include="TestVectors/jcs-ecdsa-2019-p384/sigBTC58JCSECDSAP384.txt" data-include-format="text"></pre>

        <p>Assemble the signed credential with the following two steps:</p>
        <ol>
          <li>
Add the <code>proofValue</code> field with the previously computed base58-btc
value to the proof options document.
          </li>
          <li>
Set the <code>proof</code> field of the credential to the augmented proof
option document.
          </li>
        </ol>

        <pre class="example nohighlight" title="Signed Credential"
        data-include="TestVectors/jcs-ecdsa-2019-p384/signedJCSECDSAP384.json" data-include-format="text"></pre>
      </section>
    </section>
  </body>
</html>
